<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>



  <meta name="description" content="虽然之前对计算机视觉已经有所了解，但是总是没有系统的学习过计算机视觉，所以这次的暑假学习中，将《A Practical Introduction to COMPUTER VISION WITH OPENCV》这本书放到了第一个，学习了一个月之后，在这篇博客中归纳学习到的内容，内容根据书中的顺序进行安排。">
<meta name="keywords" content="计算机视觉">
<meta property="og:type" content="article">
<meta property="og:title" content="读书笔记 - 计算机视觉入门介绍">
<meta property="og:url" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="虽然之前对计算机视觉已经有所了解，但是总是没有系统的学习过计算机视觉，所以这次的暑假学习中，将《A Practical Introduction to COMPUTER VISION WITH OPENCV》这本书放到了第一个，学习了一个月之后，在这篇博客中归纳学习到的内容，内容根据书中的顺序进行安排。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.1.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.2.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.6.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.7.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.10.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.14.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.15.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.16.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.19.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.17.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.18.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/3.1.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/3.2.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/3.3.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/3.4.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/3.5.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/3.6.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/3.9.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/3.10.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.3.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.4.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.7.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.9.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.10.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.11.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.12.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.13.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.15.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.17.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.18.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.19.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.20.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.21.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.22.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.23.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.24.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.25.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.26.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.27.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.28.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/4.29.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.1.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.2.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.3.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.4.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.5.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.6.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.7.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.8.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/5.9.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.1.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.2.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.3.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.4.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.5.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.6.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.7.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.8.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.9.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.10.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.11.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.12.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.13.png">
<meta property="og:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/6.14.png">
<meta property="og:updated_time" content="2020-08-12T01:58:48.132Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="读书笔记 - 计算机视觉入门介绍">
<meta name="twitter:description" content="虽然之前对计算机视觉已经有所了解，但是总是没有系统的学习过计算机视觉，所以这次的暑假学习中，将《A Practical Introduction to COMPUTER VISION WITH OPENCV》这本书放到了第一个，学习了一个月之后，在这篇博客中归纳学习到的内容，内容根据书中的顺序进行安排。">
<meta name="twitter:image" content="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/2.1.png">





  
  
  <link rel="canonical" href="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>读书笔记 - 计算机视觉入门介绍 | Hexo</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/06/读书笔记-计算机视觉入门介绍/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">读书笔记 - 计算机视觉入门介绍

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2020-08-06 09:49:13" itemprop="dateCreated datePublished" datetime="2020-08-06T09:49:13+08:00">2020-08-06</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2020-08-12 09:58:48" itemprop="dateModified" datetime="2020-08-12T09:58:48+08:00">2020-08-12</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/读书笔记/" itemprop="url" rel="index"><span itemprop="name">读书笔记</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>虽然之前对计算机视觉已经有所了解，但是总是没有系统的学习过计算机视觉，所以这次的暑假学习中，将《A Practical Introduction to COMPUTER VISION WITH OPENCV》这本书放到了第一个，学习了一个月之后，在这篇博客中归纳学习到的内容，内容根据书中的顺序进行安排。</p>
<a id="more"></a>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>计算机视觉是由计算机进行的，自动分析图像和视频的过程。它最初根据人类的视觉系统而来，由于人类的视觉系统几乎是出于自然的条件反射，所以最开始人们也认为计算机视觉是一个直截了当的问题，但是事实上，无论是人类的视觉系统，还是计算机视觉的难题都比想象的要更加复杂。  </p>
<h2 id="A-Difficult-Problem"><a href="#A-Difficult-Problem" class="headerlink" title="A Difficult Problem"></a>A Difficult Problem</h2><p>对于计算机视觉的研究来说，首先要知道，这是一项很难的任务，对于计算机来说，图像就是一组值，计算机视觉就是要了解（understand）这一组值，而事实就是这一组值<strong>很大</strong>，很<strong>复杂</strong>，而且<strong>时刻在变化</strong>。  </p>
<h2 id="The-Human-Vision-System"><a href="#The-Human-Vision-System" class="headerlink" title="The Human Vision System"></a>The Human Vision System</h2><p>人们对于计算机视觉，首先想到的是，能否直接复制人类的视觉系统来解决这一问题，但问题是我们目前还不清楚人类的视觉系统大多数时刻在干些啥。<br>首先对于眼睛来说，颜色视觉（6–700万个视锥细胞）集中在眼睛的中心，而其余的由1.2亿个视杆组成。所以在某种成都下，我们可以认为我们看到的，是一个连续的（continuous）图像（没有blind spot盲点），图像中各处都有颜色，但是目前我们也还不清楚这种印象如何在大脑中发生。<br>尽管我们现在能根据电子信息的活动，判断大脑的各部位负责什么任务，但是这也不能为我们提供解决计算机视觉问题的算法或思路。</p>
<h2 id="Practical-Applications-of-Computer-Vision"><a href="#Practical-Applications-of-Computer-Vision" class="headerlink" title="Practical Applications of Computer Vision"></a>Practical Applications of Computer Vision</h2><p>计算机视觉在工业中有许多应用，特别是在生产线上自动检查制成品。 例如，它已用于质量检测，生物安全识别，车牌识别等。</p>
<h2 id="The-Future-of-Computer-Vision"><a href="#The-Future-of-Computer-Vision" class="headerlink" title="The Future of Computer Vision"></a>The Future of Computer Vision</h2><p>尽管现在计算机视觉已经有很大的发展，但是仍有不足，比如现在仍然很难生产出可以在小路上行驶的可靠车辆，甚至同时也面临法律问题，如果车辆撞车，谁来负责？另外如果我们开发出一种医学成像系统来诊断癌症，而错误地没有诊断出病情时会发生什么？即使该系统可能比任何医师都更可靠，但是还是会进入法律的雷区。所以最简单的解决方案要么是仅解决非关键性问题，要么开发系统来作为当前人类专家的助手，而不是替代人类专家。另外有可能遇到的问题比如，在某些国家/地区，摄像机的安装和使用被视为侵犯了基本隐私权。<br>所以在未来，我们希望计算机视觉能在更自由更复杂的环境中发挥作用，比如自动驾驶，协助抓贼等等。<br>而计算机视觉的最终目的，是模仿人类视觉的能力，并将这些能力提供给类人机器人（和其他）机器人设备。虽然我们都有自己的良好的视觉系统，但是我们仍然希望自动的实现任何的计算机视觉任务。</p>
<h1 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h1><p>计算机视觉最重要的因素之一是图像，因为它为计算机提供了场景的视觉上的表示，以便计算机后续处理来突出感兴趣的特征，最后提取出信息。</p>
<h2 id="Cameras"><a href="#Cameras" class="headerlink" title="Cameras"></a>Cameras</h2><p>标准的相机由一个感光图像平面（photosensitive image plane）（可感应落在平面上的光量），一个防止杂散光掉落到感光图像平面上的外壳（housing），以及一个镜头罩（lens）组成，镜头允许部分光落图像平面上（镜头能通过控制透镜，将光线聚焦到图像平面上）。</p>
<h3 id="The-Simple-Pinhole-Camera-Model"><a href="#The-Simple-Pinhole-Camera-Model" class="headerlink" title="The Simple Pinhole Camera Model"></a>The Simple Pinhole Camera Model</h3><p>最简单但合理的模型是针孔照相机模型，如图2.1所示，模型中镜头化简为针孔。该模型是大多数实际成像系统的简化，因为实际系统的各个部分都会在生成的图像中引入失真（变形）。其中从3D世界中的点（x，y，z）到图像平面上的点（i，j）的映射可以建模为：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
i . w \\
j . w \\
w
\end{array}\right]=\left[\begin{array}{ccc}
f_{i} & 0 & c_{i} \\
0 & f_{j} & c_{j} \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
x \\
y \\
z
\end{array}\right]</script><p>其中$w$为尺度因子，$f_i$和$f_j$是相机的焦距和像素大小的结合，$（c_i，c_j）$是与图像平面垂直且经过针孔的线与图片平面相交的点的坐标。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.1.png" alt="图2.1"></p>
<h2 id="Images-1"><a href="#Images-1" class="headerlink" title="Images"></a>Images</h2><p>图像是由传感器捕捉到的画面（通常是3D场景的2D投影）。而且图像可以看做是图像平面上的，关于两个坐标（（i，y）或（column，row））的连续函数，但是如果要在电脑上处理这样的图像，它必须经过一下操作：</p>
<ol>
<li>采样。从传感器信息中采样，并输入一个M×N的矩阵中。</li>
<li>量化。矩阵中的每个元素都需要设置成整数，所以连续空间被分成一些间隔（通常为256）。</li>
</ol>
<h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h3><p>图像本来应该是连续的，尤其是对于人眼来说，但是对于电脑来说，它会将<strong>连续图像采样到离散的元素</strong>中来创建数字图像，以便电脑处理。而数字图像的传感器由2D的光敏元件阵列组成，且每个元件（像素点/采样点/图像点）只在<strong>一定的固定区域上是光敏的</strong>，点之间的边缘区域则不是光敏的，所以传感器会丢失掉这部分的信息（比如远处的物体恰好落在缝隙）。但采样更重要的问题是，该像素代表了离散区域的平均值（亮度/色度），这在现实世界中，可以从单个物体中折射过来，也可能是多个物体反射光的总和。<br>显而易见，像素点的数量会影响分辨率，见图2.2所示，从而限制图像中能识别出的对象，但是过多的数量又会带来超过需求的细节，使处理变的困难，因此需要权衡。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.2.png" alt="图2.2"></p>
<h3 id="Quantisation"><a href="#Quantisation" class="headerlink" title="Quantisation"></a>Quantisation</h3><p>函数$f(x，y)$表示了每个像素的亮度，亮度本应该是连续的，但是我们需要整数值，因此需要离散的表示，通常每个通道的亮度的级别有$k=2^{b}$个，其中b表示比特数（通常为8），b越大，储存图像所需的内存越大，但是越少，信息会丢失，所以实际上取决于储存的图像的目的，通常需要比预期更高的量化级别。</p>
<h2 id="Colour-Images"><a href="#Colour-Images" class="headerlink" title="Colour Images"></a>Colour Images</h2><p>灰度（单色）图像只有一个通道，只能表示每个点的亮度，而彩色（多光谱）图像具有多个通道，能同时表示场景中的亮度和色度（颜色信息）。 因此，同样的采样和量化下，彩色图像比灰度图像更大，更复杂。而我们总是需要利用彩色图像的多个通道，所以我们必须以某种方式决定如何处理每个信息通道。（也有许多图像处理是专门为灰度图像开发的。）<br>事实上是，由于人类也能理解灰度图像，且灰度图像更小更简单，所以计算机视觉多年来都一直基于灰度图像。但是颜色确实能通过更有用的信息帮助完成图像分割，物体分类等任务。<br>另外人类对于波长为400nm~700nm之间的光敏感，所以大多数相机的传感器被设计为对这些波长敏感，另外彩色图像通常使用三通道的色彩空间进行表示，如下所示。</p>
<h3 id="Red–Green–Blue-RGB-Images"><a href="#Red–Green–Blue-RGB-Images" class="headerlink" title="Red–Green–Blue (RGB) Images"></a>Red–Green–Blue (RGB) Images</h3><p>彩色图像的最常见表示形式是使用三个通道，它们大致对应于红色（700 nm），绿色（546.1 nm）和蓝色（435.8nm）波长。这意味着照相机中的感光元件对以这些颜色为中心的波长才具有光谱敏感性（如图2.6）。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.6.png" alt="图2.6"></p>
<p>值得注意的是，由于量化，尽管该色彩空间有1680万多种（256*256*256），有些颜色仍无法表示。<br>RGB颜色信息可以转换为灰度图：</p>
<script type="math/tex; mode=display">
Y=0.299 R+0.587 G+0.114 B</script><p>另外，感光平面上的一个位置只有一个元件，一个元件只对一种波长有光谱敏感性，所以对不同波长感光的元件并非位于同一位置，而是以规则模式显示，如图2.7所示，而RGB值则是以某种方式，从这些感应值内插得到。这意味着接收到的图像甚至不是连续图像的正确采样（错位），而是从传感器元件接收到的数据中插入得到的。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.7.png" alt="图2.7"></p>
<h3 id="Cyan–Magenta–Yellow-CMY-Images"><a href="#Cyan–Magenta–Yellow-CMY-Images" class="headerlink" title="Cyan–Magenta–Yellow (CMY) Images"></a>Cyan–Magenta–Yellow (CMY) Images</h3><p>CMY模型基于二次色（secondary colour，通过混合两个主次色得来）（RGB是主次色），其中C，M和Y的值，通过从纯白色中减去R，G，B的值得来。 所以，它通常在以白色为起点的打印机中用作颜色模型。</p>
<script type="math/tex; mode=display">
C=255-R, \quad M=255-G, \quad Y=255-B</script><h3 id="YUV-Images"><a href="#YUV-Images" class="headerlink" title="YUV Images"></a>YUV Images</h3><p>YUV颜色模型用于模拟电视信号（PAL，NTSC…），它由亮度（Y）以及两个颜色分量组成：蓝色减去亮度（U）和红色减去亮度（V）。 从RGB的转换方程如下：</p>
<ul>
<li>$Y=0.299 R+0.587 G+0.114 B$</li>
<li>$U=0.492^{*}(B-Y)$</li>
<li>$V=0.492^{*}(R-Y)$</li>
</ul>
<p>人类的视觉系统对亮度比对色度更加敏感，而电视信号的编码需要以减少传输的数据量为目的，所以会用到这个色彩空间。</p>
<h3 id="Hue-Luminance-Saturation-HLS-Images"><a href="#Hue-Luminance-Saturation-HLS-Images" class="headerlink" title="Hue Luminance Saturation (HLS) Images"></a>Hue Luminance Saturation (HLS) Images</h3><p>HLS模型在计算机视觉中经常使用，因为它除了将亮度和色度分开外，还将色度分为色相和饱和度，所以能够更加贴切的描述（例如深蓝色，浅红色等）。通常来说，亮度在0到1之间，色相描述了颜色，在0到360°之间，饱和度S是颜色的强度或纯度，在0到1之间。但在实际实现中，这些量会映射到到0到25​​5的范围（OpenCV内，色相值的范围是0到179）。直观的色彩空间的显示如图2.10。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.10.png" alt="图2.10"></p>
<p>从图2.10中可以明显发现，色相轴的圆形性质，这意味着最小（0）和最大（179）的色相值仅相差1，所以它们在颜色上没有太大区别，都对应于红色像素，但是单单看色相图，一个是黑色一个是白色，这意味着，如果要处理色相通道，则必须格外小心。例如，如果0、178、1、177和179的平均色相值应为179，而不是107。<br>另外，RGB和HLS之间的转换方程如下（RGB都被规范化到0.0和1.0之间）：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
L=\operatorname{Max}(R, G, B)+\operatorname{Min}(R, G, B) / 2 \\
S=\left\{\begin{array}{ll}
\operatorname{Max}(R, G, B)-\operatorname{Min}(R, G, B) / \operatorname{Max}(R, G, B)+\operatorname{Min}(R, G, B) & \text { if } L<0.5 \\
\operatorname{Max}(R, G, B)-\operatorname{Min}(R, G, B) / 2-(\operatorname{Max}(R, G, B)+\operatorname{Min}(R, G, B)) & \text { if } L \geq 0.5
\end{array}\right. \\
H=\left\{\begin{array}{ll}
60 .(G-B) / S & \text { if } R=\operatorname{Max}(R, G, B) \\
120+60 .(B-R) / S & \text { if } G=\operatorname{Max}(R, G, B) \\
240+60 .(R-G) / S & \text { if } B=\operatorname{Max}(R, G, B)
\end{array}\right.
\end{array}</script><p>根据上面的公式，L和S值将在0.0到1.0之间，H值应在0.0到360.0之间。</p>
<h3 id="Other-Colour-Spaces"><a href="#Other-Colour-Spaces" class="headerlink" title="Other Colour Spaces"></a>Other Colour Spaces</h3><p>OpenCV提供对其他六个颜色空间的支持（就转换功能而言）：</p>
<ol>
<li>HSV。</li>
<li>YCrCb 缩放了的YUV，用于图像和视频压缩。</li>
<li>CIE XYZ 标准的参考色彩空间，其中通道响应类似于人眼中不同的圆锥响应。</li>
<li>CIE ${L}^{<em>}$${u}^{</em>}$${v}^{<em>}$ CIE定义的另一种标准颜色空间，旨在提供一种感知上统一的颜色空间，其中颜色之间的差异与我们感知到的差异成比例。 $\mathrm{L}^{</em>}$是亮度的量度，$\mathrm{u}^{<em>}$和$\mathrm{v}^{</em>}$是色度值。</li>
<li>CIE $\mathrm{L}^{<em>}$ $\mathrm{a}^{</em>}$ $\mathrm{b}^{*}$ 与设备无关的色彩空间，其中包括人类可以感知的所有颜色。</li>
<li>拜耳（Bayer）是CCD传感器中广泛使用的模式，如果我们有原始传感器数据（即尚未插值），则使用拜耳模式。</li>
</ol>
<h3 id="Some-Colour-Applications"><a href="#Some-Colour-Applications" class="headerlink" title="Some Colour Applications"></a>Some Colour Applications</h3><p>在某些应用中，我们需要确定哪些像素代表特定颜色。例如，要查找路标，我们会对红色，黄色，蓝色，黑色和白色特别感兴趣。我们可以通过创建用于识别特定颜色的简单公式来识别特定颜色，如以下的介绍，但应注意的是，标准本身真的太粗糙了，很容易失效，也因为该标准不是基于足够数量的图像，所以实际上应该以更严格的方式进行处理。</p>
<h4 id="Skin-Detection"><a href="#Skin-Detection" class="headerlink" title="Skin Detection"></a>Skin Detection</h4><p>简单的通过以下公式：</p>
<script type="math/tex; mode=display">
(Saturation >=0.2 ) AND (0.5<\text { Luminance } / \text { Saturation }<3.0 ) AND (Hue <=28^{\circ} OR Hue >=330^{\circ} )</script><p>将识别许多皮肤像素。 但是，很明显，这也会识别其他像素。</p>
<h4 id="Red-Eye-Detection"><a href="#Red-Eye-Detection" class="headerlink" title="Red Eye Detection"></a>Red Eye Detection</h4><p>同样通过以下公式：</p>
<script type="math/tex; mode=display">
(Luminance >=0.25 ) AND (Saturation >=0.4 ) AND \left.\qquad(0.5<\text { Luminance/Saturation }<1.5) \text { AND (Hue }<=14^{\circ} \text { OR Hue }>=324^{\circ}\right)</script><p>识别红眼像素，通过实验可以确定，这还需加以改进，但是它是识别红眼的良好起点。</p>
<h2 id="Noise"><a href="#Noise" class="headerlink" title="Noise"></a>Noise</h2><p>图像通常会受到某种程度的噪声（噪声可以是任何会降低图像质量的因素）的影响，且这种噪声会对处理过程产生影响。总的来说，噪声能来源于环境，成像设备，电气干扰，数字化过程等。而对于噪声，我们需要能够测量它，并以某种方式对它进行校正。<br>首先对噪声大小的衡量标准，我们使用信噪比（the signal to noise ratio）来衡量。对于一幅图像$f(i, j)$，信噪比定义为：</p>
<script type="math/tex; mode=display">
S / \text {Nratio}=\Sigma_{(i, j)} f^{2}(i, j) / \Sigma_{(i, j)} v^{2}(i, j)</script><p>其中$v(i, j)$代表噪声。</p>
<h3 id="Types-of-Noise"><a href="#Types-of-Noise" class="headerlink" title="Types of Noise"></a>Types of Noise</h3><p>两种最常见的噪声类型是高斯噪声和椒盐噪声。</p>
<h4 id="Gaussian-Noise"><a href="#Gaussian-Noise" class="headerlink" title="Gaussian Noise"></a>Gaussian Noise</h4><p>高斯噪声能很好的拟合许多的实际噪声。其中，噪声$v(i, j)$被建模为一个平均值为$\mu$（通常为0），标准差为$\sigma$的高斯分布，如图2.14所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.14.png" alt="图2.14"></p>
<h4 id="Salt-and-Pepper-Noise"><a href="#Salt-and-Pepper-Noise" class="headerlink" title="Salt and Pepper Noise"></a>Salt and Pepper Noise</h4><p>椒盐噪声是脉冲噪声的一种，通过对单个像素的损坏，使其亮度与周围区域的亮度差异很大，其中饱和脉冲噪声会影响图像（即，纯白色和黑色像素会破坏图像），如图2.15所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.15.png" alt="图2.15"></p>
<h3 id="Noise-Models"><a href="#Noise-Models" class="headerlink" title="Noise Models"></a>Noise Models</h3><p>噪声必须以某种方式与图像数据结合在一起，根据噪声是与数据无关还是与数据有关可以将之分为两类。</p>
<h4 id="Additive-Noise"><a href="#Additive-Noise" class="headerlink" title="Additive Noise"></a>Additive Noise</h4><p>这类噪声模型用于数据独立的噪声（噪声量与图像数据本身无关），其中附加噪声模型能表示为：</p>
<script type="math/tex; mode=display">
f(i, j)=g(i, j)+v(i, j)</script><p>其中$g(i, j)$是原始图像，$v(i, j)$是噪声，而$f(i, j)$是实际图像。</p>
<h4 id="Multiplicative-Noise"><a href="#Multiplicative-Noise" class="headerlink" title="Multiplicative Noise"></a>Multiplicative Noise</h4><p>这类噪声模型用于数据依赖的噪声（噪声量与图像数据本身有关），其中乘法噪声模型能表示为：</p>
<script type="math/tex; mode=display">
f(i, j)=g(i, j)+g(i, j) \cdot v(i, j)</script><p>其中$g(i, j)$是原始图像，$v(i, j)$是噪声，而$f(i, j)$是实际图像。</p>
<h3 id="Noise-Generation"><a href="#Noise-Generation" class="headerlink" title="Noise Generation"></a>Noise Generation</h3><p>为了评估消除噪声的算法，我们经常需要先模拟噪声，以便可以消除/减少噪声后，评估算法成功的程度。首先假设我们生成的噪声具有高斯分布，均值为0，标准差为$\sigma$。<br>我们首先确定，能从最大可能的负变化到最大可能的正变化（通常k = -255..255）之间的所有可能噪声值的概率分布$p(k)$和累积概率分布$p_cum(k)$。</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(k) &=e^{-k^{2} / 2 \sigma^{2}} / \sigma \sqrt{2 \pi} \quad k=-(G-1), \ldots,-1,0,1, \ldots, G-1 \\
p_{\mathrm{cum}}(k) &=p_{\mathrm{cum}}(k-1)+p(k) \\
p_{\mathrm{cum}}(-(G-1)) &=p(-(G-1))
\end{aligned}</script><p>累积概率分布确定后，我们就能计算图像中，每个像素的噪声值了。<br>对于每个像素$(x，y)$，可以设置其值为：</p>
<script type="math/tex; mode=display">
f^{*}(i, j)=g(i, j)+\operatorname{argmin}_{\mathrm{k}}\left(\operatorname{rand} 0-p_{\mathrm{cum}}[k]\right)</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
f^{\prime}(x, y)=0 & \text { if } f^{*}(x, y)<0 \\
f^{\prime}(x, y)=G-1 & \text { if } f^{*}(x, y)>G-1 \\
f^{\prime}(x, y)=f^{*}(x, y) & \text { otherwise }
\end{array}</script><p>注意，argmin函数给出最小值的索引，在这种情况下，将在累积分布内选择值最接近随机数的k。另外，截断（为确保值保持在0到255之间）其实会稍微改变噪声的高斯性质。</p>
<h3 id="Noise-Evaluation"><a href="#Noise-Evaluation" class="headerlink" title="Noise Evaluation"></a>Noise Evaluation</h3><p>噪声的评估可以主观或客观地进行。 在主观评估中，将图像显示给观察者，观察者根据一系列标准对其进行评估并给出评分。但是在客观评估中，给定图像$f(i, j)$和已知参考图像$g(i, j)$，我们可以对它们之间差异进行度量，例如：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\text { Mean Quadratic Difference }=\sum \sum(g(i, j)-f(i, j))^{2} \\
\text { Mean Absolute Difference }=\sum \sum|g(i, j)-f(i, j)| \\
\text { Or we can compute the signal to noise ratio assuming} v(i, j)=f(i, j)-g(i, j)
\end{array}</script><p>显然，这种客观评估需要我们事先知道原始图像或添加的噪声。因此，实验中经常人为地添加噪声，以检验现有技术在去除噪声方面的表现如何。</p>
<h2 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h2><p>消除噪声的方法有很多种，但是由于技术原理的不同或图像数据性质的不同，在不同情况下应该使用不同的技术。<br>其中，减少噪声最常用的方法是线性平滑变换，其中的计算可以表示为线性和，但是这样的噪声会导致锐边模糊，因此有些方法提出了非线性变换（不能表示为简单的线性和，逻辑操作等）。<br>注意这些技术面对像是大斑点或者后条纹，都无法起作用，这时需要用到的是图像恢复技术。</p>
<h3 id="Image-Averaging"><a href="#Image-Averaging" class="headerlink" title="Image Averaging"></a>Image Averaging</h3><p>如果对完全相同场景，存在多个图像，则将它们进行平均处理，就可以减少噪声。比如现有n张图像，其平均值为：</p>
<script type="math/tex; mode=display">
f^{\prime}(i, j)=\frac{1}{n} \sum_{k=1 . . n} f_{k}(i, j)=\frac{1}{n} \sum_{k=1 . . n} g_{k}(i, j)+v_{k}(i, j)</script><p>这种消除噪声的算法假设噪声是数据独立的。同样也假设：</p>
<ul>
<li>所有图像的原始图像$g_{\mathrm{k}}(i, j)$是相同的（即场景和摄像机是静态的）。</li>
<li>每个图像中的噪声$v_{\mathrm{k}}(i, j)$之间存在统计独立性。</li>
<li>噪声$v_{\mathrm{k}}(i, j)$具有高斯分布，且均值为0和标准差为𝜎。</li>
</ul>
<p>如果以上假设成立，那么图像平均之后，将保持高斯性质，但标准偏差将减小$\sqrt{n}$倍，如图2.16所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.16.png" alt="图2.16"></p>
<p>由以上假设可知，该方法不适用于椒盐噪声，且当场景非静态时，将引入模糊。</p>
<h3 id="Local-Averaging-and-Gaussian-Smoothing"><a href="#Local-Averaging-and-Gaussian-Smoothing" class="headerlink" title="Local Averaging and Gaussian Smoothing"></a>Local Averaging and Gaussian Smoothing</h3><p>如果只有一个图像可用，则仍然可以执行平均，但不是全局，而是对图像中的每个点，计算以当前点为中心的像素块（而不是使用多个图像中的对应点）的平均值，最简单的比如3×3。<br>若每个点都均等权重，该技术称为局部平均（local averaging），也就是<strong>加入模糊效果</strong>，滤波器的蒙版越大，效果越明显。但是也可以更改权重，以便为更接近当前点的点赋予更高的权重，其中非常常见的加权方法是由高斯分布定义的。 例如，以下所示：</p>
<script type="math/tex; mode=display">
h_{1}=\frac{1}{9}\left[\begin{array}{lll}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1
\end{array}\right] \quad h_{2}=\frac{1}{10}\left[\begin{array}{lll}
1 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 1
\end{array}\right] \quad h_{3}=\frac{1}{16}\left[\begin{array}{lll}
1 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 1
\end{array}\right]</script><p>$h_1$是一个3×3的局部平均滤波器，$h_2$和$h_3$都是3×3的高斯平滑滤波器，但前者的$\Sigma$更小。</p>
<p>很明显，局部像素进行平均时，会引入模糊从而降低可见的（高斯）噪声，但是它也会对边缘（灰度或颜色变化很大）产生影响，反而使图像更模糊，反而更难处理，但有时这都无可厚非，都是需要平衡的点。<br>值得注意的是，对于椒盐噪声，平均后，噪声也会再次平滑到图像中，局部平均滤波器并不适用于此类噪声。<br>另外，对于这种平均操作，通常使用卷积技术来执行，将掩码（通常为正方形且以当前点为中心）与所有可能位置进行卷积：</p>
<script type="math/tex; mode=display">
f^{\prime}(i, j)=\sum_{\mathrm{m}} \sum_{\mathrm{n}} f(i, j) \cdot h(i-m, j-n)</script><h3 id="Rotating-Mask"><a href="#Rotating-Mask" class="headerlink" title="Rotating Mask"></a>Rotating Mask</h3><p>旋转遮罩是一种非线性算子，它首先分别在九个预设的包含当前点的mask内进行计算，选出其中最均匀的（即自相似的）mask，然后在mask内，应用平均滤波器。其中所有的mask如图2.19所示，其形状和大小可以变化。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.19.png" alt="图2.19"></p>
<p>对于每一个mask，我们计算与mask中的点相对应的点的局部平均值（色散）来衡量其自似性，然后选择色散最小的mask来进行平均化操作。这样的目的是减少当前点的噪声，让当前点与来自相同物理刺激（例如表面或物体）的其他类似点取平均值。<br>该技术可以迭代着来使用，直到没有变化或变化很小。掩码越大，收敛越快，但尽管该技术要慢得多，但它能同时<strong>抑制噪声和锐化图像边缘</strong>。其中色散可通过计算每个点的平方差的均值得来：</p>
<script type="math/tex; mode=display">
D=\frac{1}{n} \sum_{(i, j) \in M a s k}\left(f(i, j)-1 / n \sum_{\left(i^{\prime}, j^{\prime}\right) \in \text { Mask }} f\left(i^{\prime}, j^{\prime}\right)\right)^{2}</script><p>化简为；</p>
<script type="math/tex; mode=display">
D=\frac{1}{n}\left(\sum_{(i, j) \in \text { Mask }} f(i, j)^{2}-1 / n\left(\sum_{\left(i^{\prime}, j^{\prime}\right) \in \text { Mask }} f\left(i^{\prime}, j^{\prime}\right)\right)^{2}\right)</script><p>化简后，计算cost显着降低。<br>另外，该技术可以应用于，具有椒盐噪声的图像，但是会导致不良效果，尤其是当噪声存在于物体边缘时。</p>
<h3 id="Median-Filter"><a href="#Median-Filter" class="headerlink" title="Median Filter"></a>Median Filter</h3><p>另一种非线性平滑操作是将每个像素替换为，以该像素为中心的小区域（例如3x3）中的像素中位值。例如，3x3区域包含像素值（25 21 23 25 18 18 255 30 13 22），排序后为（13 18 22 21 23 25 25 30 255），其中位数为23，而均值为48。可见该技术非常擅长处理噪声，且该技术不会使边缘模糊太多，所以可迭代应用。而实际上，中值滤波与旋转mask的效果非常相似。如图2.17和2.18所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.17.png" alt="图2.17"></p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/2.18.png" alt="图2.18"></p>
<p>虽然也可将其与上一个，在非矩形区域计算的技术结合，但是它还是会损坏细线和角，而且其时间复杂度为$O\left(k^{2} \log k\right)$，所以计算上也很昂贵。拓展的优化技术可看<a href="http://vision.gel.ulaval.ca/~perreaul/publications/Id699_2007.pdf" target="_blank" rel="noopener">Median Filtering in Constant Time by Perreault</a></p>
<h1 id="Histograms"><a href="#Histograms" class="headerlink" title="Histograms"></a>Histograms</h1><p>一个图像的直方图是对该图像的抽象描述，其中包含了各个图像值（亮度/强度）的频率。</p>
<h2 id="1D-Histograms"><a href="#1D-Histograms" class="headerlink" title="1D Histograms"></a>1D Histograms</h2><p>对于灰度图像，其中灰度强度有256种（0-255），而每个强度对应的值，表示图像中属于该灰度的，有多少个像素。如图3.1所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/3.1.png" alt="图3.1"></p>
<p>图像的直方图中包含了该图像的全局信息，并且该信息完全独立于场景中各个对象的位置和方向，所以同一直方图对应的图像不是唯一的，许多非常不同的图像可能具有相似（甚至相同）的直方图。另外，直方图中得出的信息（例如平均强度及其标准偏差）可用于执行分类。</p>
<h3 id="Histogram-Smoothing"><a href="#Histogram-Smoothing" class="headerlink" title="Histogram Smoothing"></a>Histogram Smoothing</h3><p>直方图中，无论是全局还是局部的最大值和最小值都能提供有用的信息，但是局部的极值太多了。为了减少此数字，可以对直方图进行平滑处理。首先创建一个新的直方图，其中每个值，都是以原始直方图中的相应值为中心的多个值的平均值。此过程通常称为过滤。如图3.2。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/3.2.png" alt="图3.2"></p>
<h3 id="Colour-Histograms"><a href="#Colour-Histograms" class="headerlink" title="Colour Histograms"></a>Colour Histograms</h3><p>对于彩色图像这样的多通道图像来说，通常为每个通道独立确定直方图，如图3.3，图3.4。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/3.3.png" alt="图3.3"></p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/3.4.png" alt="图3.4"></p>
<p>由上可知，不同的颜色模型，直方图也有很大的区别，进而对其有用性产生巨大影响。比如图3.4中可以直接发现，红色和绿色很显著。</p>
<h2 id="3D-Histograms"><a href="#3D-Histograms" class="headerlink" title="3D Histograms"></a>3D Histograms</h2><p>对于色彩图像的直方图处理，若是均独立处理，则不会实现最佳的颜色分割。比如相似颜色的点都具有较高的饱和度和色相值，这时需要更好的分割，则需要用到3D直方图，如图3.5。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/3.5.png" alt="图3.5"></p>
<p>其中，(0,0,0)点显示在左下角的顶层。绿轴从顶层的左下角到右下角，蓝轴从顶层的左下角到左上角，红轴从顶层的左下角到最底层的左下角。每个单元内显示的灰度强度表示每个单元中的相对像素数（黑色= 0，白色=最大）。<br>另外就是直方图中的<strong>单元数的设置</strong>。如果我们假设每个通道8位，则直方图中将有近1,680万个单元，这作为图像中信息的摘要来说太多了，所以我们需要减少了直方图的量化。比如图3.5中，每个通道只有2位，所以直方图中只有64个单元。<br>OpenCV中的代码可写为：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MatND histogram;</span><br><span class="line"><span class="keyword">int</span> channel_numbers[] = &#123; <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span> &#125;;</span><br><span class="line"><span class="keyword">int</span> * number_bins = <span class="keyword">new</span> <span class="keyword">int</span>[image.channels()]; </span><br><span class="line"><span class="keyword">for</span> (ch=<span class="number">0</span>; ch &lt; image.channels(); ch++)</span><br><span class="line">	number_bins[ch] = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">float</span> ch_range[] = &#123; <span class="number">0.0</span>, <span class="number">255.0</span> &#125;;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">float</span> * channel_ranges[] = &#123;ch_range,ch_range,ch_range&#125;;</span><br><span class="line">calcHist( &amp;image, <span class="number">1</span>, channel_numbers, Mat(), histogram, image.channels(), a_number_bins, channel_ranges );</span><br></pre></td></tr></table></figure></p>
<h2 id="Histogram-Image-Equalisation"><a href="#Histogram-Image-Equalisation" class="headerlink" title="Histogram/Image Equalisation"></a>Histogram/Image Equalisation</h2><p>在最佳观看条件下，人类可以区分700到900种灰度，但是在图像的非常黑或非常亮的区域，just noticeable difference（JND）将大大降低，人类也将很难解读图像，然而，很明显，人类更容易区分较大的差异，因此，如果改善图像中的灰度分布，这将有助于人类观察者的理解。<br>而改善图像中的灰度分布的一种技术是<strong>直方图均衡化</strong>，它通过在图像中均匀分布灰度，使生成的直方图变得平坦（即各个灰度具有完全相同的点数），但绝对的平坦是不可能的，实际情况是，有些像素没有值，而有较高值的灰度图散落在直方图中。见图3.6。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/3.6.png" alt="图3.6"></p>
<p>另外，对于彩色图像时，我们通常仅均衡亮度通道的直方图，以免颜色失真。</p>
<h2 id="Histogram-Comparison"><a href="#Histogram-Comparison" class="headerlink" title="Histogram Comparison"></a>Histogram Comparison</h2><p>大多数图像搜索引擎都提供了，检索与给定图像相似或包含特定内容的图像的功能，这大多数情况下是通过使用与图像关联的元数据标签实现的，但是通过分析图像中的颜色分布，也可以为该过程提供帮助，比如分析比较直方图。<br>用于比较直方图的指标如：</p>
<ul>
<li>$D_{\text {Correlation }}\left(h_{1}, h_{2}\right)=\frac{\sum_{i}\left(h_{1}(i)-\overline{h_{1}}\right)\left(h_{2}(i)-\overline{h_{2}}\right)}{\sqrt{\sum_{i}\left(h_{1}(i)-\overline{h_{1}}\right)^{2} \sum_{i}\left(h_{2}(i)-\overline{h_{2}}\right)^{2}}}$</li>
<li>$D_{\text {Chi }-\text { Square }}\left(h_{1}, h_{2}\right)=\sum_{i} \frac{\left(h_{1}(i)-h_{2}(i)\right)^{2}}{\left(h_{1}(i)+h_{2}(i)\right)}$</li>
<li>$D_{\text {Intersection }}\left(h_{1}, h_{2}\right)=\sum_{i} \min \left(h_{1}(i), h_{2}(i)\right)$</li>
<li>$D_{\text {Bhattacharyya }}\left(h_{1}, h_{2}\right)=\sqrt{1-\frac{1}{\sqrt{\overline{h_{1}} \overline{h_{2}} N^{2}}} \sum_{i} \sqrt{h_{1}(i) \cdot h_{2}(i)}}$<br>其中:</li>
<li>$N$ 是直方图中的格子数(bin)</li>
<li>$\overline{h_{k}}=\Sigma_{i}\left(h_{k}(i)\right) / N$</li>
</ul>
<p>另外，也可以使用<strong>Earth Mover’s Distance</strong>，来确定将一种分布（在这种情况下为直方图）转换为另一种分布的最低路线的距离。通过迭代可以得出结果：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
E M D(-1)=0 \\
E M D(i)=h_{1}(i)+E M D(i-1)-h_{2}(i)
\end{array}</script><p>最后的距离就可写为：</p>
<script type="math/tex; mode=display">
\sum_{i}|E M D(i)|</script><p>这也可以用在颜色直方图中。</p>
<h2 id="Back-projection"><a href="#Back-projection" class="headerlink" title="Back-projection"></a>Back-projection</h2><p>上一章介绍了一些非常简单的方法来选择图像中的特定颜色。这些方法在定义它们的颜色空间中选择了相当粗糙的子空间。解决此问题的更好方法如：</p>
<ol>
<li>获取要选择的颜色的代表性样本集（类似颜色的照片切片）。</li>
<li>根据这些样本创建直方图。</li>
<li>将直方图标准化，使其最大值为1.0，这样就可以将直方图的值视为概率（即，具有相应颜色的像素来自样本集的概率）。</li>
<li>将归一化后的直方图反向投影到，需要计算图像中的每个像素与样本集的像素的相似度的图像中，也就是得出了一个概率图$p$，其中$p(i, j)$表示$f(i, j)$像素和对应样本集之间的相似性，即$p(i, j)=h(f(i, j)))$。</li>
</ol>
<h2 id="k-means-Clustering"><a href="#k-means-Clustering" class="headerlink" title="k-means Clustering"></a>k-means Clustering</h2><p>颜色的一个主要问题是它种类太多了（例如8位RGB空间就有1680万种颜色），所以通常会希望减少此数字，来更真实的压缩图像，或者表示某人穿着的衣服的颜色。为了实现这种效果，一种常用的技术是在3D颜色空间中进行聚类（例如，参见图3.9中使用的<strong>原始的k均值聚类算法</strong>）。  </p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/3.9.png" alt="图3.9"></p>
<p>该算法的目的，是找出k个示例（即特定颜色）来最好地表示图像中的所有颜色，这个k是超参数，是预先指定的。图像中的每个像素的颜色称为pattern、图案。而与特定的exemplar、范例（k个示例之一）关联的一组pattern，称为一簇。操作如下：</p>
<ul>
<li>起始exemplar可以来源于：<ul>
<li>随意选择</li>
<li>选择最开始的k个pattern</li>
<li>均匀的分布</li>
</ul>
</li>
<li>第一轮：对于所有pattern，将其分配给最近的exemplar。在每个新pattern都被关联之后，计算与exemplar相关联的所有pattern的重心，并将之作为新的exemplar。</li>
<li>第二遍：使用第一遍得到的新的exemplar，然后给所有pattern根据距离重新分配给exemplar（这些分配可能与上次不同）。 </li>
</ul>
<p>对k聚类算法的简单的一维展示如图3.10所示：</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/3.10.png" alt="图3.10"></p>
<p>注意，由于每次起始样本的选择都是随机的，所有该<strong>算法是不确定</strong>的，所有每次执行都会有不同的结果。另外对于一个区域，用多少种颜色来表示并不容易，比如在图3.9中，只有右侧的图像才能真实地<strong>再现</strong>每个斯诺克球的颜色，所以如何才能确定，用多少个颜色（k）再现区域会更加合适呢？<br>一种方法是看哪个聚类数的输出结果具有最高的置信度，置信度可以使用诸如Davies-Bouldin指数之类的指标：</p>
<script type="math/tex; mode=display">
\mathrm{DB}=1 / \mathrm{k} \sum_{1 . . \mathrm{k}} \max _{i \neq j}\left(\left(\Delta_{\mathrm{i}}+\Delta_{\mathrm{j}}\right) / \delta_{\mathrm{i} . \mathrm{j}}\right)</script><p>它考虑了k个聚类，并对聚类之间的分离度求和。具体来说，对于每个簇，都使用以上公式，找到与之最不分离的另一个簇。以上公式对于任何两个簇，分别计算两个簇到各自的簇中心的平均距离之和（Δi +Δj），再除以簇之间的距离，得到分离度。尽管此指标是簇中最常用的指标之一，但由于它没有考虑簇的大小，因此在簇的大小不一的情况下效果不佳。</p>
<p>总的来说，k均值聚类是无监督学习的一个示例，即从现有的数据中学会了划分方式。其中非监督学习是指学习过程中，不会收到分类是否正确的反馈的学习，它必须基于<strong>输入数据的内部规律</strong>，来让相似的事件/像素点/物体，以相同方式被分类。</p>
<h1 id="Binary-Vision"><a href="#Binary-Vision" class="headerlink" title="Binary Vision"></a>Binary Vision</h1><p>灰度图像的每个像素通常有8位，尽管在某些方面，它处理起来会比彩色图像更容易，但是存在一种比灰度图像更简单的图像形式（即二进制图像）。实际上，计算机视觉已经有很大一部分的实际应用基于二元图像。  </p>
<h2 id="Thresholding"><a href="#Thresholding" class="headerlink" title="Thresholding"></a>Thresholding</h2><p>通过对灰度图像进行阈值处理，能得到二元图像。算法很简单：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{For all pixe} & \text{ls} (i, j) \\
f^{\prime}(i, j) &=1 \text { where } f(i, j)>=T \\
&=0 \text { where } f(i, j)<T
\end{aligned}</script><p>二元图像的灰度值并非二进制的0和1，而通常使用灰度值0和255代替，所以可以使用8位的格式表示结果图像，进而可以用与原始灰度图像相同的方式来显示和处理。<br>虽然可以逐元素判断其值是否超过阈值，但最有效的方法（通常可以在硬件中完成）是使用查找表。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{For all grey lev} &\text{els } k = 0 \text{. . .} 255 \\
\operatorname{LUT}(k) &=1 \text { where } k>=T \\
&=0 \text { where } k<T \\
\text{For all pixels } (i&, j) \\
g(i, j) = &\operatorname{LUT}(f(i, j))
\end{aligned}</script><p>通常使用阈值操作以便将某些感兴趣的对象与背景分离，且通常将感兴趣的部分用1（或255）表示，但有时需要反转二进制图像才能如此。</p>
<h3 id="Thresholding-Problems"><a href="#Thresholding-Problems" class="headerlink" title="Thresholding Problems"></a>Thresholding Problems</h3><p>若要使用阈值技术，首先，要分离开的前景和背景必须是不同的。如果它们没有区别，将很难（甚至不可能）使用阈值技术来准确地对其进行细分。但也有许多技术（比如自适应阈值）来尝试处理前景和背景的区别不明确的情况，和许多技术（例如腐蚀，膨胀，打开，关闭） 来改善分割不完美的二值图像。</p>
<h2 id="Threshold-Detection-Methods"><a href="#Threshold-Detection-Methods" class="headerlink" title="Threshold Detection Methods"></a>Threshold Detection Methods</h2><p>即使在某些工业化的密闭场景，随着时间的推移，其照明条件也会发生变化，其光源也会随着时间逐渐变弱，若只是凭借一开始手动设置的阈值，就可能会引起问题。因此，需要一种机制来自动的确定阈值。<br>在以下内容中，我们假设有一个灰度图像$f(i, j)$，它的直方图为$h(g)=\sum_{i, j}\left[\begin{array}{l}1 f(i, j)=g \\ 0 \text { otherwise }\end{array}\right.$，再将之转换为概率分布$p(g)=h(g) / \sum_{g} h(g)$。</p>
<h3 id="Bimodal-Histogram-Analysis"><a href="#Bimodal-Histogram-Analysis" class="headerlink" title="Bimodal Histogram Analysis"></a>Bimodal Histogram Analysis</h3><p>图像的阈值可以通过分析其直方图而来。现在我们假设，图像的背景主要集中于一个灰度值，而前景主要集中于另一个灰度值，那进而可以假设直方图是双峰的（即具有两个主峰）。然后，要计算阈值，我们可以简单地寻找anti-mode、反模式（峰值之间的最小值）。  </p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.3.png" alt="图4.3"></p>
<p>但是，如图4.3所示，虽然直方图整体上是双峰的，但它同时也存在许多局部的最大值和最小值，所以要找到反模式其实并不容易。一般有以下几种方法来解决此问题时：</p>
<ul>
<li>首先使直方图平滑（来抑制噪声的峰值）</li>
<li>使用可变的步长来找反模式（而不是将每个直方图的每个模式都考虑到）</li>
<li>忽略梯度高的点（因为这些点代表了边界），从而更好的分割直方图的两种模式</li>
<li>仅仅对边界点绘制直方图，然后直接获取直方图的模式，即为反模式。</li>
</ul>
<p>然而，以上这些方法有时会造成反模式的偏移，从而产生不好的阈值。所以接下来介绍其他基于直方图的更为可靠的方法。</p>
<h3 id="Optimal-Thresholding"><a href="#Optimal-Thresholding" class="headerlink" title="Optimal Thresholding"></a>Optimal Thresholding</h3><p>以上的技术，适用于模式合理地分开并噪声不太大的情况，但是当模式之间的距离越来越近时，首先反模式就不再是最佳解决方案。如图4.4中的两个正态分布及其求和所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.4.png" alt="图4.4"></p>
<p>其最佳阈值应该是两个正态分布相交的位置（左侧垂直线），但这时总分布的反模式位于右侧（右侧垂直线）。<br>综上所述，如果我们可以将直方图，建模为两个正态分布（但可能重叠）的总和，那么可以使用一种称为“Optimal Thresholding、最佳阈值”的算法，该算法没有直观的解释其原理，但它不断的迭代，将产生最终结果。<br>算法如下：</p>
<ol>
<li>设置$t$为0，$T^t$=&lt;一些初始的阈值&gt;</li>
<li>根据当前阈值$T^t$，计算前景和背景的平均值<script type="math/tex; mode=display">
\begin{array}{ll}
w_{\mathrm{b}}\left(T^{t}\right)=\sum_{g=0}^{T^{\prime}-1} p(g) \quad \mu_{\mathrm{b}}\left(T^{\prime}\right)=\Sigma_{g=0}^{T^{\prime}-1} p(g) \cdot g / w_{\mathrm{b}}\left(T^{\prime}\right) \\
w_{\mathrm{f}}\left(T^{t}\right)=\sum_{g=T^{\prime}}^{255} p(g)=1-w_{\mathrm{b}}\left(T^{t}\right) \quad \mu_{\mathrm{f}}\left(T^{\prime}\right)=\sum_{g=T^{\prime}}^{255} p(g) \cdot g / w_{\mathrm{f}}\left(T^{\prime}\right)
\end{array}</script></li>
<li>设置$T^{t+1}=\left(\mu_{\mathrm{b}}\left(T^{t}\right)+\mu_{\mathrm{f}}\left(T^{t}\right)\right) / 2$以及将t增加1，为下一迭代更新阈值</li>
<li>回到第二步，直到$T^{t+1}=T^{t}$</li>
</ol>
<p>重要的是，设置的初始值必须能让某些对象和某些背景像素分离，否则算法将因除零错误而失败。另外，只有当直方图满足“两个正态分布的和的”假设时，该算法输出的结果才是最佳的选择。</p>
<h3 id="Otsu-Thresholding"><a href="#Otsu-Thresholding" class="headerlink" title="Otsu Thresholding"></a>Otsu Thresholding</h3><p>Optimal Thresholding 所假设的条件，通常不会成立，这时它的结果可能不可接受，所以Otsu定义了另一种方法，将阈值两边的像素值的分布最小化：  </p>
<p>考虑所有可能的阈值，并选择能使类方差$\sigma_{W}^{2}(T)$最小的阈值T：  </p>
<script type="math/tex; mode=display">
\sigma_{W}^{2}(T)=w_{\mathrm{f}}(T) \sigma_{f}^{2}(T)+w_{\mathrm{b}}(T) \sigma_{b}^{2}(T)</script><p>其中$w_{\mathrm{f}}(T)$和$w_{\mathrm{b}}(T)$分别是属于前景和背景部分的点，$\sigma_{f}^{2}(T)$和$\sigma_{b}^{2}(T)$分别是前景和背景灰度值的方差：</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
w_{f}(T)=\sum_{g=T}^{255} p(g) & \sigma_{f}^{2}(T)=\sum_{g=T}^{255} p(g) \cdot\left(g-\mu_{f}(T)\right)^{2} / w_{\mathrm{f}}(T) \\
w_{\mathrm{b}}(T)=\sum_{g=0}^{T-1} p(g) & \sigma_{b}^{2}(T)=\sum_{g=0}^{T-1} p(g) \cdot\left(g-\mu_{b}(T)\right)^{2} / w_{\mathrm{b}}(T)
\end{array}</script><p>其中$\mu_{f}(T)$和$\mu_{b}(T)$分别是前景和背景的灰度值的均值。  </p>
<script type="math/tex; mode=display">
\mu_{f}(T)=\sum_{g=T}^{255} p(g) \cdot g / w_{\mathrm{f}}(T) \quad \mu_{b}(T)=\sum_{g=0}^{T-1} p(g) \cdot g / w_{\mathrm{b}}(T)</script><p>让类里方差最小的阈值也是能让类之间方差$\sigma_{B}^{2}(T)$最大的阈值，这是因为$\sigma_{B}^{2}(T)=\sigma^{2}-\sigma_{W}^{2}(T)$，其中$\mu$和$\sigma^{2}$是图像数据的均值和方差。<br>给两个类$f$和$b$，其类之间方差为：</p>
<script type="math/tex; mode=display">
\sigma_{B}^{2}(T)=w_{f}(T)\left(\mu_{f}(T)-\mu\right)^{2}+w_{b}(T)\left(\mu_{b}(T)-\mu\right)^{2}</script><p>进而简化为：</p>
<script type="math/tex; mode=display">
\sigma_{B}^{2}(T)=w_{f}(T) w_{b}(T)\left(\mu_{f}(T)-\mu_{b}(T)\right)^{2}</script><p>其中$\mu=w_{f}(T) \mu_{f}(T)+w_{b}(T) \mu_{b}(T)$<br>所以总的来说，就是遍历所有可能的阈值T，找到能使上式的类间方差最大的阈值，即为结果，可以看出我们只需计算前后部分的权重和均值，就能作出判断。</p>
<h2 id="Variations-on-Thresholding"><a href="#Variations-on-Thresholding" class="headerlink" title="Variations on Thresholding"></a>Variations on Thresholding</h2><h3 id="Adaptive-Thresholding"><a href="#Adaptive-Thresholding" class="headerlink" title="Adaptive Thresholding"></a>Adaptive Thresholding</h3><p>以上出现的技术中，都对全局进行阈值处理（即，将单个阈值应用于图像中的所有点）。但在某些情况下，也可以使用多个阈值来更好的改善阈值结果。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.7.png" alt="图4.7"></p>
<p>比如图4.7，其中若只用单个的全局最佳阈值，图中的大部分书面细节将消失。而使用自适应阈值处理（具有64个阈值/图像块），大多数细节都可以正确显示。其中，自适应阈值算法为：</p>
<ol>
<li>将图像划分为多个子图像（如在图4.7中，用8×8网格分割成64个子图像）。</li>
<li>对于每个子图像，分别计算阈值。</li>
<li>对于图像中的每个点，使用双线性插值从四个最近的阈值中插值一个阈值来确定该点的阈值。</li>
</ol>
<p>图4.7中可发现，该技术并非在所有地方都能正常工作，比如有两个黑色区域，对于这种情况，可以给全局的各个阈值添加限制来解决，以确保阈值在整个图像没有明显的变化。<br>重要的是，在OpenCV中，该技术并非本文介绍的那样，它计算的是当前像素，与以当前像素为中心的，<em>block_size</em>大小的像素块的局部平均值之间的差异，如果差异减去<em>offset</em>之后比0大，则设置输出值为255，反而设置为0，其中<em>block_size</em>对输出有很大影响，另外，也可以使用高斯加权平均值来代替局部平均值。实际上，基于局部区域的均值，相当于以另一种方式得到了当前像素的新阈值。</p>
<h3 id="Band-Thresholding"><a href="#Band-Thresholding" class="headerlink" title="Band Thresholding"></a>Band Thresholding</h3><p>Band Thresholding使用了两个阈值，一个在对象/物体的像素以下，另一个在对象/物体的像素以上：</p>
<script type="math/tex; mode=display">
\begin{array}{ralign}
\text{For all pixels } & (i, j) \\
f^{\prime}(i, j)&= 1 \text { for } f(i, j) \geq T_{1} \text { and } f(i, j) \leq T_{2}\\
&= 0 \text { otherwise}
\end{array}{}</script><p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.9.png" alt="图4.9"></p>
<p>如图4.9所示，该技术可以用于定位物体/对象的边界，虽然对于边界，边缘检测器会更可靠，更合适。</p>
<h3 id="Semi-Thresholding"><a href="#Semi-Thresholding" class="headerlink" title="Semi-Thresholding"></a>Semi-Thresholding</h3><p>半阈值化，对物体像素保留其原始灰度值，但将其背景像素设置为黑色。</p>
<script type="math/tex; mode=display">
\begin{array}{ralign}
\text{For all pixels } & (i, j) \\
f^{\prime}(i, j)&=f(i, j) \text { for } f(i, j) \geq T\\
&= 0 \text { otherwise}
\end{array}{}</script><p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.10.png" alt="图4.10"></p>
<p>结果图如图4.10所示。</p>
<h3 id="Multispectral-Thresholding"><a href="#Multispectral-Thresholding" class="headerlink" title="Multispectral Thresholding"></a>Multispectral Thresholding</h3><p>对于彩色图像，如果要应用阈值，最常见的做法是将图像转换为灰度，然后转换为阈值，不过也可以对每个通道独立应用阈值（如图4.11所示），甚至还可以在3D颜色空间内进行阈值设置（比如，如果一个像素的颜色，存在于3D色彩空间的特定子空间中，则将该像素定义为对象/物体像素）。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.11.png" alt="图4.11"></p>
<h2 id="Mathematical-Morphology"><a href="#Mathematical-Morphology" class="headerlink" title="Mathematical Morphology"></a>Mathematical Morphology</h2><p>数学形态学是一种，基于set operations、运算集的图像处理方法，这些运算集通常对物体的形状，进行非线性算子的代数运算。它提供了许多描述图像处理和分析操作的统一框架，并让一些很难描述的技术（以下内容）的开发成为可能。<br>其中，<strong>形态运算</strong>将structuring elements、结构元素有效地移动到图像中的每个可能的位置，并分别进行逻辑运算（以某种方式将结构元素与图像进行比较），并将运算结果存储在单独的输出图像中。  </p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.12.png" alt="图4.12"></p>
<p>操作过程中，将二值图像看作2D的网格，对象点就是网格中的点，且网格原点与原图的原点相同（如图4.12（a）所示）。而结构元素通常是围绕它们自己的原点（通常对称）定义的少量的对象点集。典型的结构元素是各向同性的（矩形中的所有点都属于结构元素），比如图4.12（b）和（c），但它不必是各向同性的，如图4.12（d）。</p>
<h3 id="Dilation"><a href="#Dilation" class="headerlink" title="Dilation"></a>Dilation</h3><p>膨胀是一种用于扩展物体/对象的像素数量的技术，如图4.13所示，它通常会同时在所有方向上扩展：</p>
<script type="math/tex; mode=display">
X \oplus B=\left\{p \in \varepsilon^{2} ; p=x+b, x \in X \text { and } b \in B\right\}</script><p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.13.png" alt="图4.13"></p>
<p>它将二值图像X中的每一个对象点x，转换为结构元素B中的每一个对象点b（相对于结构元素原点的对象点向量），所以图像X中的每一个对象点都能造成输出图像中出现一些对象点，但重复项不保留。<br>膨胀会增加物体的大小（即集合中的点数），从外观上来看，它会导致小孔被填充，并将较大的对象块之间的狭窄间隙填满。另外，一般用于图像中的扩散使用各向同性的结构元素。</p>
<h3 id="Erosion"><a href="#Erosion" class="headerlink" title="Erosion"></a>Erosion</h3><p>侵蚀是一种通过消除边界上的像素点，来缩小对象/物体形状的技术，如图4.15所示：</p>
<script type="math/tex; mode=display">
X \ominus B=\left\{p \in \varepsilon^{2} ; p+b \in X \text { for every } b \in B\right\}</script><p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.15.png" alt="图4.15"></p>
<p>由图可知，当且仅当为当前点转换时，结构元素所对应的所有点都是对象点，该点才是侵蚀输出集的元素，也可将其视为匹配问题，将结构元素与输入图像的每个可能位置进行匹配，只有完美匹配的点，才能被标记输出。<br>此操作会减少了对象的大小（即集合中的点数），即，消除任何小的噪声点，以及任何狭窄的特征。与膨胀类似，侵蚀一般也使用各向同性的结构元素。</p>
<h3 id="Opening-and-Closing"><a href="#Opening-and-Closing" class="headerlink" title="Opening and Closing"></a>Opening and Closing</h3><p>侵蚀可以看做是膨胀的镜像，反之亦然，一个扩大对象像素的数量，另一个缩小对象像素的数量，将这些操作结合在一起，就是开运算和闭运算。<br>开运算是先侵蚀再膨胀的操作，使用的结构元素相同：</p>
<script type="math/tex; mode=display">
X \mathrm{O} B=(X \ominus B) \oplus B</script><p>开运算可以消除噪点（消除比结构元素小的图像细节），以及较窄的特征（例如较大对象块之间的连接），并平滑对象边界。与侵蚀和膨胀不同，它能近似保持物体的大小。<br>而闭运算是先进行膨胀操作，再进行侵蚀操作，使用的结构元素相同：</p>
<script type="math/tex; mode=display">
X \cdot B=(X \oplus B) \ominus B</script><p>闭运算能连接互相接近的对象，并填充对象内的孔。它倾向于使对象的形状有些变形，但也能近似保持物体的大小。<br>在许多应用中，通常会先进行一个闭运算然后进行一个开运算，来搭理二值图像，如图4.17和图4.18。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.17.png" alt="图4.17"></p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.18.png" alt="图4.18"></p>
<h3 id="Grey-Scale-and-Colour-Morphology"><a href="#Grey-Scale-and-Colour-Morphology" class="headerlink" title="Grey-Scale and Colour Morphology"></a>Grey-Scale and Colour Morphology</h3><p>除了二值图像，形态学运算也可以应用于灰度和彩色图像。在这些情况下，各个通道上的每个不同的灰度层级（level）都被单独视为一个集合（即，所有大于或等于<strong>特定灰度级别</strong>的点），如图4.19所示。图4.20和图4.21，分别是对灰度图像以及颜色图像进行开运算和闭运算的示例。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.19.png" alt="图4.19"></p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.20.png" alt="图4.20"></p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.21.png" alt="图4.21"></p>
<p>具体来说，可以理解为，对给定像素进行结构像素的匹配，找到对应区域的最小值或最大值，膨胀的公式如下：</p>
<script type="math/tex; mode=display">
\delta_{B}(f)_{x}=(f \oplus B)_{x}=\max _{\beta \in B} f(x+\beta)</script><p>侵蚀的公式如下：</p>
<script type="math/tex; mode=display">
\varepsilon_{B}(f)_{x}=(f \ominus B)_{x}=\min _{\beta \in B} f(x+\beta)</script><p>开运算和闭运算不再赘述。</p>
<h2 id="Connectivity"><a href="#Connectivity" class="headerlink" title="Connectivity"></a>Connectivity</h2><p>在进行阈值处理以及噪声清理后，就需要在场景中定位对象了，而定位则需要确定哪些像素实际连在一起，如图4.22所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.22.png" alt="图4.22"></p>
<h3 id="Connectedness-Paradoxes-and-Solutions"><a href="#Connectedness-Paradoxes-and-Solutions" class="headerlink" title="Connectedness: Paradoxes and Solutions"></a>Connectedness: Paradoxes and Solutions</h3><p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.23.png" alt="图4.23"></p>
<p>见上图左边，可以发现，两条线虽然相交，表现为连续的，但是并不属于同一部分，而右边可以发现，两个似乎连续的两个圆，在同样似乎连续的背景上。这两个图展示了可能会遇到的一些问题，虽然在二值图中，右边的图才更有意义。<br>在确定哪些像素相邻时，首先要选择两种方案：4邻接，仅将东、南、西、北的像素视为相邻；以及8邻接，将所有直接围绕的像素视为相邻，见图4.24所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.24.png" alt="图4.24"></p>
<p>从理论上讲，我们希望使用像素的相邻性，来构建连续的整块区域，既区域中的任意两个点都可以用属于该区域的点连接起来。此类区域，相对于背景而言也就是对象/物体（有可能含有孔）。但是对于图4.23中的圆来说，无论是4邻接还是8邻接都无法得到预期的结果，如图4.25所示。  </p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.25.png" alt="图4.25"></p>
<p>解决此问题的一种思路是，同时使用4邻接和8邻接：</p>
<ul>
<li>对于外部背景使用4邻接原则。 </li>
<li>对于外部物体/对象使用8邻接原则。</li>
<li>对于孔使用4邻接原则。</li>
<li>对于孔中的物体使用8邻接原则。</li>
</ul>
<p>效果如图4.26所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.26.png" alt="图4.26"></p>
<h3 id="Connected-Components-Analysis"><a href="#Connected-Components-Analysis" class="headerlink" title="Connected Components Analysis"></a>Connected Components Analysis</h3><p>在解决了连通性悖论后，需要一种实用的算法来给每个像素贴标签，这里的像素仅指二值图像中的对象像素，所以不用担心连接性问题，故可以仅使用8邻接和4邻接的其中一个来为像素贴标签（因为基于先前的相邻像素），见图4.27。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.27.png" alt="图4.27"></p>
<p>算法如下：</p>
<ol>
<li>在图像中逐行搜索，并在每行里逐列搜索：<br> 对于每个非零的像素：<pre><code> 如果所有先前的相邻像素，都是背景像素：
     给当前像素赋予新的标签
 否则：
     从先前的相邻像素的标签中,任选一个作为当前像素的标签
     如果任何两个先前的相邻像素之间有不同的标签：
         将这些标签记下为等效的
</code></pre></li>
<li>遍历整个图像，将等效的标签都设置为相同的标签值。</li>
</ol>
<p>算法说明见图4.28，效果见4.29。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.28.png" alt="图4.28"></p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/4.29.png" alt="图4.29"></p>
<p>总的来说，我们用阈值来处理图像时，能将之分成不同的两个区域，再使用CCA（连接成分分析），能让我们走的更远，并能标记连接起来的二元区域，而分出来的区域会更加多且细。<br>另外，通常情况是会同时标记物体的点和背景的点，所以应该对物体使用8邻原则，对背景/孔使用4邻原则。</p>
<h1 id="Geometric-Transformations"><a href="#Geometric-Transformations" class="headerlink" title="Geometric Transformations"></a>Geometric Transformations</h1><p>图像处理中常常用到几何变换（操作），比如将多个图像带入同一参照系中，以便组合或比较，也可用于消除失真和畸变来使像素间距变得均匀，甚至可以用来简化之后的处理过程，如图5.1中，使平面物体的图像与图像内的轴对齐后，能方便后序处理。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.1.png" alt="图5.1"></p>
<h2 id="Problem-Specification-and-Algorithm"><a href="#Problem-Specification-and-Algorithm" class="headerlink" title="Problem Specification and Algorithm"></a>Problem Specification and Algorithm</h2><p>给定一个变形的图像$f(i, j)$和一个校正的图像$f^{\prime}\left(i^{\prime}, j^{\prime}\right)$，我们可以将其坐标之间的几何变换建模为$i=T_{i}\left(i^{\prime}, j^{\prime}\right)$和$j=T_{j}\left(i^{\prime}, j^{\prime}\right)$，值得注意的是，这里的公式让我们在已知校正图像的坐标后，计算出对应的畸变图像的坐标。<br>在变换图像前，我们必须事先定义变换的方式，比如变换可以是已知的，也可以是通过失真的样本图像和校正的样本图像之间的一对对的对应点来决定的，对于后面一种情况，主要有两种方案来确定对应关系：</p>
<ul>
<li>通过对已知的一个图案（例如图5.2）进行成像，获取失真后的图像，而校正后的图像可以直接从已知图案中获取。</li>
<li>获取同一物体的两幅图像，其中一个图像（称为失真图像）将被映射给另一幅图像（称为校正图像），作为参考的框架。</li>
</ul>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.2.png" alt="图5.2"></p>
<p>一旦获取了充分的样本失真图像和样本校正图像之间的对应关系，就可以相对简单地计算几何变换函数了，而变换一旦确立，就可以将其应用于失真的图像以及需要相同类型的“校正”的任何其他图像（比如设置固定的相机，其拍摄的所有图像都将具有相同的失真）。<br>    几何变换的应用方式如下：</p>
<ul>
<li>对于输出/校正图像$\left(i^{\prime}, j^{\prime}\right)$的每一点：<ul>
<li>使用$T_{i}\left(\right)$和$T_{j}\left(\right)$，来计算它从哪个坐标$(i, j)$变换而来。</li>
<li>根据输入图像中的邻近点，为输出点插值，其中$T_{i}\left(\right)$和$T_{j}\left(\right)$有可能计算出小数，所以坐标有可能在像素点之间。</li>
</ul>
</li>
</ul>
<p>对于变换，按照一般图像处理的顺序，应该是对于输入图像的每一点，计算其对应输出点的值，但是这样可能会造成输出图像中的部分像素缺失，所以通常的做法是将其反向应用，对每一个输出点，都从输入图像中为其计算一个值。但反向执行计算的<strong>唯一缺点</strong>是，我们将在变形失真的图像的域内进行插值，而不是在校正后的图像域内。但通常这样做的偏差不大。</p>
<h2 id="Affine-Transformations"><a href="#Affine-Transformations" class="headerlink" title="Affine Transformations"></a>Affine Transformations</h2><p>一般的变换都可使用仿射（Affine）变换来描述：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{l}
i \\
j
\end{array}\right]=\left[\begin{array}{lll}
a_{00} & a_{01} & a_{02} \\
a_{10} & a_{11} & a_{12}
\end{array}\right]\left[\begin{array}{l}
i^{\prime} \\
j^{\prime} \\
1
\end{array}\right]</script><p>仿射变换通常用于简单且常规的变换，例如缩放，旋转等，但也可基于样本的映射（变形和校正图像的点之间的对应关系）来确定未知的变换。</p>
<h3 id="Known-Affine-Transformations"><a href="#Known-Affine-Transformations" class="headerlink" title="Known Affine Transformations"></a>Known Affine Transformations</h3><p>以下是一些常见的已知变换：</p>
<h4 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h4><script type="math/tex; mode=display">
\left[\begin{array}{l}
i \\
j
\end{array}\right]=\left[\begin{array}{lll}
1 & 0 & m \\
0 & 1 & n
\end{array}\right]\left[\begin{array}{l}
i^{\prime} \\
j^{\prime} \\
1
\end{array}\right]</script><p>该方程描述的变换是沿水平轴平移m个单位，沿垂直轴平移n个单位。</p>
<h4 id="Change-of-Scale-Expand-Shrink"><a href="#Change-of-Scale-Expand-Shrink" class="headerlink" title="Change of Scale (Expand/Shrink)"></a>Change of Scale (Expand/Shrink)</h4><script type="math/tex; mode=display">
\left[\begin{array}{l}
i \\
j
\end{array}\right]=\left[\begin{array}{lll}
a & 0 & 0 \\
0 & b & 0
\end{array}\right]\left[\begin{array}{l}
i^{\prime} \\
j^{\prime} \\
1
\end{array}\right]</script><p>该方程描述的变换是在水平轴上缩放比例a，在垂直轴上缩放比例b，这么做通常是为了规范图像中的对象的大小。</p>
<h4 id="Rotation"><a href="#Rotation" class="headerlink" title="Rotation"></a>Rotation</h4><script type="math/tex; mode=display">
\left[\begin{array}{l}
i \\
j
\end{array}\right]=\left[\begin{array}{ccc}
\cos \phi & \sin \phi & 0 \\
-\sin \phi & \cos \phi & 0
\end{array}\right]\left[\begin{array}{l}
i^{\prime} \\
j^{\prime} \\
1
\end{array}\right]</script><p>该方程描述的变换是绕原点旋转角度𝜙，它可使图像与轴对齐以便简化后续处理，见图5.3。</p>
<h4 id="Skewing"><a href="#Skewing" class="headerlink" title="Skewing"></a>Skewing</h4><script type="math/tex; mode=display">
\left[\begin{array}{l}
i \\
j
\end{array}\right]=\left[\begin{array}{ccc}
1 & \tan \phi & 0 \\
0 & 1 & 0
\end{array}\right]\left[\begin{array}{l}
i^{\prime} \\
j^{\prime} \\
1
\end{array}\right]</script><p>该方程描述的变换是将图像<strong>倾斜</strong>角度𝜙，能用来消除非线性效果，例如线性扫描相机生成的效果，见图5.3。</p>
<h4 id="Panoramic-Distortion"><a href="#Panoramic-Distortion" class="headerlink" title="Panoramic Distortion"></a>Panoramic Distortion</h4><script type="math/tex; mode=display">
\left[\begin{array}{l}
i \\
j
\end{array}\right]=\left[\begin{array}{lll}
a & 0 & 0 \\
0 & 1 & 0
\end{array}\right]\left[\begin{array}{l}
i^{\prime} \\
j^{\prime} \\
1
\end{array}\right]</script><p>全景失真/变形是指图像的宽高比不正确，当镜片（mirror）以不正确的速度旋转时，在线性扫描仪中就会出现，见图5.3。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.3.png" alt="图5.3"></p>
<h3 id="Unknown-Affine-Transformations"><a href="#Unknown-Affine-Transformations" class="headerlink" title="Unknown Affine Transformations"></a>Unknown Affine Transformations</h3><p>在很多情况下，要用到的变换是未知的，但是仍然可以使用刚才提到的仿射变换来描述它。然后问题就等效成了求仿射变换中的六个未知参数$a_{00} \ldots a_{12}$。因为是六个未知数，所以我们至少需要三对观测值来确定这些系数，当然观测值越多，得到的变换方程就更精准。具体来说，如果观测到的映射为：$\left(i_{1}, j_{1}\right) \leftrightarrow\left(i_{1}^{\prime}, j_{1}^{\prime}\right)$，$\left(i_{2}, j_{2}\right) \leftrightarrow\left(i_{2}^{\prime}, j_{2}^{\prime}\right)$和$\left(i_{3}, j_{3}\right) \leftrightarrow\left(i_{3}^{\prime}, j_{3}^{\prime}\right)$，则对于：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{l}
i \\
j
\end{array}\right]=\left[\begin{array}{lll}
a_{00} & a_{01} & a_{02} \\
a_{10} & a_{11} & a_{12}
\end{array}\right]\left[\begin{array}{c}
i^{\prime} \\
j^{\prime} \\
1
\end{array}\right]</script><p>我们可以重新排列为：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
i_{1} \\
j_{1} \\
i_{2} \\
j_{2} \\
i_{3} \\
j_{3}
\end{array}\right]=\left[\begin{array}{cccccc}
i_{1}^{\prime} & j_{1}^{\prime} & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & i_{1}^{\prime} & j_{1}^{\prime} & 1 \\
i_{2}^{\prime} & j_{2}^{\prime} & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & i_{2}^{\prime} & j_{2}^{\prime} & 1 \\
i_{3}^{\prime} & j_{3}^{\prime} & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & i_{3}^{\prime} & j_{3}^{\prime} & 1
\end{array}\right]\left[\begin{array}{c}
a_{00} \\
a_{01} \\
a_{02} \\
a_{10} \\
a_{11} \\
a_{12}
\end{array}\right]</script><p>将两边都乘以正方矩阵的逆，就能求解系数，如图5.4所示。但是如果有更多的观测值，那么矩阵将不是正方形，我们必须使用伪逆。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.4.png" alt="图5.4"></p>
<h2 id="Perspective-Transformations"><a href="#Perspective-Transformations" class="headerlink" title="Perspective Transformations"></a>Perspective Transformations</h2><p>大多数摄像机都是通过透视投影来形成图像的，因为3D世界的光线都是通过单个点（小孔成像）或镜头投影到图像平面上。但如果观察的平面与相机的平面不平行时，仅依靠仿射变换，将无法正确校准该平面的试图（如图5.4仿射变换校正的车牌的右下方），所以我们就需要更复杂的变换模型：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
i \cdot w \\
j \cdot w \\
w
\end{array}\right]=\left[\begin{array}{ccc}
p_{00} & p_{01} & p_{02} \\
p_{10} & p_{11} & p_{12} \\
p_{20} & p_{21} & 1
\end{array}\right]\left[\begin{array}{c}
i^{\prime} \\
j^{\prime} \\
1
\end{array}\right]</script><p>对于此变换，我们至少需要四对观测点，可通过以下所示计算出系数：<br>其中$i \cdot w=p_{00} \cdot i^{\prime}+p_{01} \cdot j^{\prime}+p_{02}$以及$w=p_{20} \cdot i^{\prime}+p_{21} \cdot j^{\prime}+1$<br>所以$i=p_{00} \cdot i^{\prime}+p_{01} \cdot j^{\prime}+p_{02}-p_{20} \cdot i \cdot i^{\prime}-p_{21} \cdot i \cdot j^{\prime}$<br>同样的，可得出$j=p_{10} \cdot i^{\prime}+p_{11} \cdot j^{\prime}+p_{12}-p_{20} \cdot j \cdot i^{\prime}-p_{21} \cdot j \cdot j^{\prime}$。  </p>
<p>所以使用四对观察之后可得到：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
i_{1} \\
j_{1} \\
i_{2} \\
j_{2} \\
i_{3} \\
j_{3} \\
i_{4} \\
j_{4}
\end{array}\right]=\left[\begin{array}{cccccccc}
i_{1}^{\prime} & j_{1}^{\prime} & 1 & 0 & 0 & 0 & -i_{1} i_{1}^{\prime} & -i_{1} j_{1}^{\prime} \\
0 & 0 & 0 & i_{1}^{\prime} & j_{1}^{\prime} & 1 & -j_{1} i_{1}^{\prime} & -j_{1} j_{1}^{\prime} \\
i_{2}^{\prime} & j_{2}^{\prime} & 1 & 0 & 0 & 0 & -i_{2} i_{2}^{\prime} & -i_{2} j_{2}^{\prime} \\
0 & 0 & 0 & i_{2}^{\prime} & j_{2}^{\prime} & 1 & -j_{2} i_{2}^{\prime} & -j_{2} j_{2}^{\prime} \\
i_{3}^{\prime} & j_{3}^{\prime} & 1 & 0 & 0 & 0 & -i_{3} i_{3}^{\prime} & -i_{3} j_{3}^{\prime} \\
0 & 0 & 0 & i_{3}^{\prime} & j_{3}^{\prime} & 1 & -j_{3} i_{3}^{\prime} & -j_{3} j_{3}^{\prime} \\
i_{4}^{\prime} & j_{4}^{\prime} & 1 & 0 & 0 & 0 & -i_{4} i_{4}^{\prime} & -i_{4} j_{4}^{\prime} \\
0 & 0 & 0 & i_{4}^{\prime} & j_{4}^{\prime} & 1 & -j_{4} i_{4}^{\prime} & -j_{4} j_{4}^{\prime}
\end{array}\right]\left[\begin{array}{c}
p_{00} \\
p_{01} \\
p_{02} \\
p_{10} \\
p_{11} \\
p_{12} \\
p_{20} \\
p_{21}
\end{array}\right]</script><p>与仿射变换相似，我们也可以通过将两边乘以方矩阵的逆，来确定系数。效果如图5.4和图5.5所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.5.png" alt="图5.5"></p>
<h2 id="Specification-of-More-Complex-Transformations"><a href="#Specification-of-More-Complex-Transformations" class="headerlink" title="Specification of More Complex Transformations"></a>Specification of More Complex Transformations</h2><p>仿射变换和透视变换都是线性变换，都可以通过矩阵来计算，但在某些情况下（例如，将不同时间或用不同设备拍摄的两个医学图像对齐），就需要更复杂的转换。</p>
<p>这些变换通常使用，项目数量预定义的多项式来近似表示：</p>
<script type="math/tex; mode=display">
\begin{aligned}
i=T_{i}\left(i^{\prime}, j^{\prime}\right)=& a_{00}+a_{10} i^{\prime}+a_{01} j^{\prime}+a_{11} i^{\prime} j^{\prime}+a_{02}\left(j^{\prime}\right)^{2}+a_{20}\left(i^{\prime}\right)^{2}+a_{12} i^{\prime}\left(j^{\prime}\right)^{2}+a_{21}\left(i^{\prime}\right)^{2} j^{\prime} \\
&+a_{22}\left(i^{\prime}\right)^{2}\left(j^{\prime}\right)^{2}+\ldots \\
j=T_{j}\left(i^{\prime}, j^{\prime}\right)=& b_{00}+b_{10} i^{\prime}+b_{01} j^{\prime}+b_{11} i^{\prime} j^{\prime}+b_{02}\left(j^{\prime}\right)^{2}+b_{20}\left(i^{\prime}\right)^{2}+b_{12} i^{\prime}\left(j^{\prime}\right)^{2}+b_{21}\left(i^{\prime}\right)^{2} j^{\prime} \\
&+b_{22}\left(i^{\prime}\right)^{2}\left(j^{\prime}\right)^{2}+\ldots
\end{aligned}</script><p>同样的，为了求解多项式的系数，也需要知道失真和校正图像之间的一些对应关系，其最小数量由多项式的阶数定义（至少是项数的一半），解决方程式中所涉及的数学与前两个相似，而为了准确的计算系数：</p>
<ul>
<li>找到的对应关系的点必须在整个图像上均匀分布；</li>
<li>不能包括不正确的对应关系；</li>
<li>使用的对应点越多越好；</li>
<li>对应关系的点的位置，应该尽量以最高的精度表示；</li>
<li>当计算逆矩阵或伪逆矩阵时，必须非常小心地处理矩阵，因为可能会存在数值不稳定的问题（因为大数和小数被组合在一起运算，而其中小数的准确性对于结果至关重要）。</li>
</ul>
<p>如果几何运算太复杂而无法用多项式近似，可将图像进行分区，然后为每个分区确定一个变换来近似估计图像。</p>
<h2 id="Interpolation"><a href="#Interpolation" class="headerlink" title="Interpolation"></a>Interpolation</h2><p>对于变换函数映射出的失真图像中的坐标而言，它不太可能精确地对应于任何一个像素，所以我们需要根据对应坐标附近的像素值来进行插值。插值有很多方案，这里详细介绍三种：</p>
<h3 id="Nearest-Neighbour-Interpolation"><a href="#Nearest-Neighbour-Interpolation" class="headerlink" title="Nearest Neighbour Interpolation"></a>Nearest Neighbour Interpolation</h3><script type="math/tex; mode=display">
f^{\prime}\left(i^{\prime}, j^{\prime}\right)=f(\text { round }(i), \text { round}(j))</script><p>在最近点插值法中，仅对真实坐标进行四舍五入，取最近的像素点的值。虽然简单，但这种方案会经常导致非常明显的块状效应，如图5.6。特别是对于具有直线边界的对象而言，会更明显，出现像阶梯一样的效果，因此很少使用。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.6.png" alt="图5.6"></p>
<h3 id="Bilinear-Interpolation"><a href="#Bilinear-Interpolation" class="headerlink" title="Bilinear Interpolation"></a>Bilinear Interpolation</h3><script type="math/tex; mode=display">
\begin{aligned}
f^{\prime}\left(i^{\prime}, j^{\prime}\right)=&(\operatorname{trunc}(i)+1-i)(\operatorname{trunc}(j)+1-j) f(\operatorname{trunc}(i), \operatorname{trunc}(j)) \\
&+(i-\operatorname{trunc}(i))(\operatorname{trunc}(j)+1-j) f(\operatorname{trunc}(i)+1, \operatorname{trunc}(j)) \\
&+(\operatorname{trunc}(i)+1-i)(j-\operatorname{trunc}(j)) f(\operatorname{trunc}(i), \operatorname{trunc}(j)+1) \\
&+(i-\operatorname{trunc}(i))(j-\operatorname{trunc}(j)) f(\operatorname{trunc}(i)+1, \operatorname{trunc}(j)+1)
\end{aligned}</script><p>该插值方案基于四个最接近的相邻像素$f(t r u n c(i), t r u n c(j)), f(t r u n c(i)+1, t r u n c(j)), f(t r u n c(i), t r u n c(j)+1), f(t r u n c(i)+1, t r u n c(j)+1)$，根据它们与实际点$(i, j)$的距离，加权求和得来，权重与它们到实际点$(i, j)$的绝对距离成比例关系（介于 0.0和1.0），另外该插值方案假定亮度函数是双线性的，所以权重分别取了水平和竖直方向上的距离的乘积，另外，由于离得越近权重越高，所以权重使用了实际点到对称的点的距离的值，以提供与该点的实际距离的反向衡量。<br>线性插值克服了最邻点插值所引起的块效应，但是会轻微的降低分辨率以及造成模糊，如图5.7所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.7.png" alt="图5.7"></p>
<h3 id="Bi-Cubic-Interpolation"><a href="#Bi-Cubic-Interpolation" class="headerlink" title="Bi-Cubic Interpolation"></a>Bi-Cubic Interpolation</h3><p>双三次插值不仅克服了最近点插值中出现的阶梯状边界问题，也解决了双线性插值的模糊问题。它通过使用双三次多项式曲面上的16个邻近点，来局部估计，改善插值。该函数与拉普拉斯算子（能用于锐化图像）非常相似。<br>双三次插值经常在栅格显示中使用，这就是为什么图像在显示屏上极具放大后很少会出现块状的原因，如5.8所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.8.png" alt="图5.8"></p>
<h2 id="Modelling-and-Removing-Distortion-from-Cameras"><a href="#Modelling-and-Removing-Distortion-from-Cameras" class="headerlink" title="Modelling and Removing Distortion from Cameras"></a>Modelling and Removing Distortion from Cameras</h2><p>在照相机系统（即照相机和镜头）中发生的几何变形和校正，是计算机视觉中常见的几何变形和校正形式之一。<br>针孔相机提供了一个照相机系统的简化模型，但实际上，许多真实的摄像机系统都无法使用该模型进行精确建模，因此还需要对由于物理设置以及镜头引起的失真进行建模。</p>
<h3 id="Camera-Distortions"><a href="#Camera-Distortions" class="headerlink" title="Camera Distortions"></a>Camera Distortions</h3><p>相机系统中经常会出现两种形式的畸变：（i）径向畸变（辐射型变形）和（ii）切向畸变。<br>Radial distortion，<strong>径向畸变</strong>是径向（放射状）对称的畸变，其畸变的程度与该点到相机光轴（通常在图像中心附近）的距离有关。这种畸变实际上是由于放大倍数在变化。如果放大倍数随着与光轴的距离的增加而减小，该变形应称为barrel distortion，桶状畸变(广角)。 另一方面，放大倍数反而增加了，则称为pincushion distortion，枕状畸变（长焦）。<br>如果我们假设图像$f(i, j)$的原点就在光轴上，则：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
i^{\prime}=i\left(1+k_{1} r^{2}+k_{2} r^{4}+k_{3} r^{6}\right) \\
j^{\prime}=j\left(1+k_{1} r^{2}+k_{2} r^{4}+k_{3} r^{6}\right)
\end{array}</script><p>其中$f^{\prime}\left(i^{\prime}, j^{\prime}\right)$是校正的图像，$r=\sqrt{i^{2}+j^{2}}$是到光轴的距离，$k_{1}, k_{2}, k_{3}$是形容该畸变的参数。</p>
<p>Tangential distortion，<strong>切向畸变</strong>是当透镜未完全平行于成像平面时会发生的畸变，它会导致放大倍率不均匀，在这种情况下，放大倍率会从成像平面的一侧到另一侧变化，再次假设图像的原点在光轴上，则可以将切向畸变建模为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
i^{\prime}=i+\left(2 p_{1} i j+p_{2}\left(r^{2}+2 i^{2}\right)\right) \\
j^{\prime}=j+\left(2 p_{2} i j+p_{1}\left(r^{2}+2 j^{2}\right)\right)
\end{array}</script><p>其中$r=\sqrt{i^{2}+j^{2}}$是到光轴的距离，$p_{1}, p_{2}$是形容该畸变的参数。</p>
<h3 id="Camera-Calibration-and-Removing-Distortion"><a href="#Camera-Calibration-and-Removing-Distortion" class="headerlink" title="Camera Calibration and Removing Distortion"></a>Camera Calibration and Removing Distortion</h3><p>为了确定是哪些参数造成的成像系统的各种畸变，有必要对其进行校准。通常，这意味着同时确定畸变参数和摄像机型号。首先给相机系统展示一个已知的校准对象，如图5.9所示，并不断的变换位置和方向。然后可以使用，与之前从样本点中确定仿射变换和透视变换过程中使用到的，类似的数学方法来提取模型参数。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/5.9.png" alt="图5.9"></p>
<p>校准后，只需根据公式将其变形，即可消除该相机拍摄的图像中的任何畸变。</p>
<h1 id="Edges"><a href="#Edges" class="headerlink" title="Edges"></a>Edges</h1><p>图像的分割是指将图像分成多个部分的过程，以便分离不同的对象，或将对象的各个部分分离，是理解图像内容（<strong>image understanding</strong>的目标）中必不可少的步骤之一。解决图像分割主要有两种方法：</p>
<ol>
<li>边缘处理，即识别图像中的不连续性（边缘）。</li>
<li>区域处理，我们在图像中寻找同质区域（或部分），其中最简单的示例比如二元视觉，当然也有更为复杂的方法。</li>
</ol>
<p>这些表示可以，也应该是互补的，即边缘能描绘出同质区域。但事实上，很难确定边缘在哪里，因为如果它是可确定的，意味着边缘图像要是唯一的，但是对于大多数图像，没有绝对正确的答案，因为答案通常是主观的，这取决于观察者，也取决于分割的目的，所以基于边缘的视觉的许多技术的目标，都是根据边缘或区域确定场景的最佳表示。</p>
<h2 id="Edge-Detection"><a href="#Edge-Detection" class="headerlink" title="Edge Detection"></a>Edge Detection</h2><p>边缘检测通常是针对单个通道（灰度）图像展开的，其中边缘是指亮度突然变化的位置，所以我们通常从2D导数（即变化率）的角度来考虑它。另外，因为图像是处于<strong>离散域</strong>中，所以尽管亮度变化通常发生在像素之间而不是在具体像素上，我们还是将亮度突然变化的位置视为边缘像素，其中，sobel边缘检测的示例如图6.1所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.1.png" alt="图6.1"></p>
<p>由上图，我们也可以发现，每个边缘像素，既有梯度大小/gradient（变化率），也有方向/orientation（最快增长的方向），如图6.2所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.2.png" alt="图6.2"></p>
<p>通常，要么使用一阶导数运算符，要么使用二阶导数运算符（或者两者的某种组合）来执行边缘检测。一阶导数能得到边缘上的局部最大值（变化率最大的地方），二阶导数能得到边缘上的“跨零点（zero-crossing）”（函数从正无穷到负无穷，或者反过来的地方），如图6.3所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.3.png" alt="图6.3"></p>
<p>这里值得一提的是，导数是运用在连续的函数的，但是图像是在离散域中处理的，所以为了近似的估计导数（第一、第二），我们使用图像像素之间的差值。</p>
<h3 id="First-Derivative-Edge-Detectors"><a href="#First-Derivative-Edge-Detectors" class="headerlink" title="First Derivative Edge Detectors"></a>First Derivative Edge Detectors</h3><p>一阶导数的边缘检测器（其实有很多，本文只介绍几个效果明显的）先计算出两个偏导数，然后再将它们组合起来，以确定每个边缘点的梯度和方向值。最后的梯度结果，必须进行非最大值抑制以及阈值处理。</p>
<h4 id="Roberts-Edge-Detector"><a href="#Roberts-Edge-Detector" class="headerlink" title="Roberts Edge Detector"></a>Roberts Edge Detector</h4><p>该边缘检测器的两个偏导数考虑了对角线上的像素差异：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\delta_{1}(i, j)=f(i, j)-f(i+1, j+1) \\
\delta_{2}(i, j)=f(i+1, j)-f(i, j+1)
\end{array}</script><p>此类函数通常是用与图像卷积的滤波器来表示的，以便在整个图像上计算该函数（将卷积滤波器移至图像中每个可能的位置，与对应的像素进行卷积操作）:</p>
<script type="math/tex; mode=display">
\delta_{1}(i, j)=\left[\begin{array}{rr}
1 & 0 \\
0 & -1
\end{array}\right] \quad \delta_{2}(i, j)=\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right]</script><p>而梯度则以两个偏导数的RMS（Root-Mean-Square，均方根）或绝对差之和来计算。至于方向，则使用反正切tan函数计算。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.4.png" alt="图6.4"></p>
<p>对于二进制图像（图6.4），Roberts算子得到的结果非常好，很干净（其中的点要么是边缘点要么不是），且边缘只有一个像素宽。其实在本文中，Roberts是唯一一个，能用在，也是应该用在二进制图像上的一阶边缘检测器。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.5.png" alt="图6.5"></p>
<p>对于灰度图像（图6.5），Roberts算子得到的结果似乎很差。主要因为在真实图像中，边缘对应的变化不会突然出现（在两个像素之间突然出现），而是在几个像素上完成变化，而Roberts算子是基于相邻点的。另外也因为图像中的任何噪声都能严重影响其偏导数的计算。<br>另外，计算出的梯度，只是根据I轴和J轴的二分之一个像素计算而来的，其中的中点位于两个偏导数之间，正由于偏导数的这种相交方式，Roberts运算符通常被称为Roberts cross-operator、Roberts交叉运算符。</p>
<h4 id="Compass-Edge-Detectors"><a href="#Compass-Edge-Detectors" class="headerlink" title="Compass Edge Detectors"></a>Compass Edge Detectors</h4><p>Compass边缘检测器中，最著名的两个例子是Sobel和Prewitt，它们分别定义了八个偏导数，其中Prewitt定义的八个偏导数为：</p>
<script type="math/tex; mode=display">
h_{1}(i, j)=\left[\begin{array}{rrr}
1 & 1 & 1 \\
0 & 0 & 0 \\
-1 & -1 & -1
\end{array}\right] \quad h_{2}(i, j)=\left[\begin{array}{rrr}
0 & 1 & 1 \\
-1 & 0 & 1 \\
-1 & -1 & 0
\end{array}\right] \quad h_{3}(i, j)=\left[\begin{array}{ccc}
-1 & 0 & 1 \\
-1 & 0 & 1 \\
-1 & 0 & 1
\end{array}\right]</script><script type="math/tex; mode=display">
h_{4}(i, j)=\left[\begin{array}{rrr}
-1 & -1 & 0 \\
-1 & 0 & 1 \\
0 & 1 & 1
\end{array}\right] \quad h_{5}(i, j)=\left[\begin{array}{rrr}
-1 & -1 & -1 \\
0 & 0 & 0 \\
1 & 1 & 1
\end{array}\right] \quad h_{6}(i, j)=\left[\begin{array}{rrr}
0 & -1 & -1 \\
1 & 0 & -1 \\
1 & 1 & 0
\end{array}\right]</script><script type="math/tex; mode=display">
h_{7}(i, j)=\left[\begin{array}{rrr}
1 & 0 & -1 \\
1 & 0 & -1 \\
1 & 0 & -1
\end{array}\right] \quad h_{8}(i, j)=\left[\begin{array}{rrr}
1 & 1 & 0 \\
1 & 0 & -1 \\
0 & -1 & -1
\end{array}\right]</script><p>在这8个偏导数中，只有两个正交的偏导数是真正需要的$h_1$和$h_3$。示例见图6.6，其中（b）代表水平偏导数，（c）代表垂直偏导数。另外，由于偏导数可以是正数或负数，所以图中灰色表示0，白色表示最大正值，黑色表示最大负值。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.6.png" alt="图6.6"></p>
<p>而至于Sobel，对应的偏导数为：</p>
<script type="math/tex; mode=display">
h_{1}(i, j)=\left[\begin{array}{rrr}
1 & 2 & 1 \\
0 & 0 & 0 \\
-1 & -2 & -1
\end{array}\right] \quad h_{3}(i, j)=\left[\begin{array}{ccc}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{array}\right]</script><p>分别用于Sobel或Prewitt的两个偏导数，都可用于计算梯度和方向。如图6.7所示，其中（c）是所有点的方向图，（d）是阈值处理的梯度，（e）是边缘点的方法，另外，由于方向是个圆形的方向，其中$0^{\circ}$是黑色，$1^{\circ}$到$359^{\circ}$是白色。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.7.png" alt="图6.7"></p>
<p>Sobel和Prewitt都在偏导数的卷积mask中，有效地整合了平滑因子，另外，它们也考虑了那些稍微分开的点，这两个因素共同提高了它们在真实的灰度图像上的表现（与Roberts算子的表现相比）。另外这两种算子的偏导数都以特定像素为中心，因此它确定的边缘的位置不会发生偏移（而Roberts算子会导致1/2个像素的偏移）。<br>Sobel和Prewitt之间的唯一区别，是在偏导数两边的平滑滤波器的权重，所以它们之间的结果差异不大。</p>
<h4 id="Computing-Gradient-and-Orientation"><a href="#Computing-Gradient-and-Orientation" class="headerlink" title="Computing Gradient and Orientation"></a>Computing Gradient and Orientation</h4><p>一阶导数的边缘检测器，通过将两个<strong>正交</strong>的偏导数结合（通常是Root-Mean-Square，也称为$l^{2}$ <em>-norm</em>、$l^{2}$ 范数），来计算边缘的梯度：</p>
<script type="math/tex; mode=display">
\nabla f(i, j)=\sqrt{\left(\frac{\delta f(i, j)}{\delta i}\right)^{2}+\left(\frac{\delta f(i, j)}{\delta j}\right)^{2}}</script><p>出于速度原因，有时使用偏导数的绝对值之和来近似估算：</p>
<script type="math/tex; mode=display">
\nabla f(i, j)=\left|\frac{\delta f(i, j)}{\delta i}\right|+\left|\frac{\delta f(i, j)}{\delta j}\right|</script><p>至于边缘点的方向，则使用带有两个参数的反正切函数计算出（以区分所有360度）：</p>
<script type="math/tex; mode=display">
\phi(i, j)=\arctan \left(\frac{\delta f(i, j)}{\delta j}, \frac{\delta f(i, j)}{\delta i}\right)</script><p>在计算机中应使用atan2，对编程更方便。</p>
<h4 id="Edge-Image-Thresholding"><a href="#Edge-Image-Thresholding" class="headerlink" title="Edge Image Thresholding"></a>Edge Image Thresholding</h4><p>边缘图像的阈值化是应用于梯度图像的二元阈值操作。但事实上，边缘有几个像素宽，且这些像素通常都会导致梯度响应，因此阈值处理后，在边缘也会产生多个响应，如图6.8所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.8.png" alt="图6.8"></p>
<p>因此，我们需要在对其阈值处理之前，添加一个额外的阶段，在该阶段中，中心最大值以外的所有梯度响应（对于任何给定的边缘点）都将被抑制，也就是non-maxima suppression、非最大值抑制。</p>
<h4 id="Non-maxima-Suppression"><a href="#Non-maxima-Suppression" class="headerlink" title="Non-maxima Suppression"></a>Non-maxima Suppression</h4><p>首先是要确定哪些边缘点是中心点（即沿边缘轮廓上的每个点的主要响应），该算法是通过梯度大小和方向信息来判断的。通过当前点的方向，它能知道当前点的两边是哪些点，如果该点的梯度小于任何一边的梯度，则抑制该点，示例见图6.9，详细介绍见图6.10。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.9.png" alt="图6.9"></p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.10.png" alt="图6.10"></p>
<p>具体算法：</p>
<ul>
<li>量化边缘方向（将所有方向分类为八个方向，因为仅考虑八个边界点）</li>
<li>对所有的点$(i, j)$<ul>
<li>寻找与边正交（垂直）的两个点</li>
<li>如果gradient$(i, j)$&lt;这两点的任何一个梯度<ul>
<li>output $(i, j)=0$</li>
<li>否则output $(i, j)=\operatorname{gradient}(i, j)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Second-Derivative-Edge-Detectors"><a href="#Second-Derivative-Edge-Detectors" class="headerlink" title="Second Derivative Edge Detectors"></a>Second Derivative Edge Detectors</h3><p>二阶导数的边缘检测器，得到的是变化率的变化率，但实际上也是使用单个卷积滤波器在图像的离散域中进行卷积计算。其中最常见的二阶导数滤波器之一是拉普拉斯算子，它的两个离散估计分别为：</p>
<script type="math/tex; mode=display">
h(i, j)=\left[\begin{array}{rrr}
0 & 1 & 0 \\
1 & -4 & 1 \\
0 & 1 & 0
\end{array}\right]</script><p>或者:</p>
<script type="math/tex; mode=display">
h(i, j)=\left[\begin{array}{rrr}
2 & -1 & 2 \\
-1 & -4 & -1 \\
2 & -1 & 2
\end{array}\right]</script><p>值得注意的是，这些估计给了中心像素很大的权重，所以如果存在噪声时会出现问题。因此，通常在拉普拉斯滤波之前，先进行某种类型的平滑处理。<br>二阶导数边缘检测器可以用于确定梯度的大小和位置，但是它不能用于确定梯度的方向。而且就算是梯度大小，也需要知道在zero crossing处的二阶导数函数的斜率，这有点复杂，因此经常使用一阶导数函数来求梯度大小。那二阶导数边缘算子还有什么用途呢，实际上它的目的是高精度的确定边缘位置。如接下来所展示的两种算子。</p>
<h4 id="Laplacian-of-Gaussian"><a href="#Laplacian-of-Gaussian" class="headerlink" title="Laplacian of Gaussian"></a>Laplacian of Gaussian</h4><p>高斯拉普拉斯算子现在仍是使用最广泛的检测器之一。如前所述，二阶导数的边缘检测器易受噪声影响，因此需要在它之前通过平滑处理来降低噪声水平。 平滑滤波器必须满足两个条件：（i）它是平滑的，并且在频域中受频带限制（即它必须限制图像中边缘出现的频率）；（ii）它表现出良好的空间定位性（即平滑滤波器禁止移动任何边缘，或更改它们之间的空间关系）。 解决此问题的最佳方法是使用高斯平滑，其中$\sigma^{2}$表示高斯函数的宽度：</p>
<script type="math/tex; mode=display">
G(i, j)=e^{-\frac{i^{2}+j^{2}}{2 \sigma^{2}}}</script><p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.11.png" alt="图6.11"></p>
<p>为了计算二阶导数，应首先将高斯平滑算子应用于图像，然后再应用拉普拉斯算子，如图6.11所示。但是，可以将这两个运算符组合在一起，并将它们作为单个“高斯拉普拉斯算子”同时应用于图像。此运算符的卷积过滤器定义如下：</p>
<script type="math/tex; mode=display">
h(i, j)=\frac{1}{\pi \sigma^{4}}\left(\frac{i^{2}+j^{2}}{2 \sigma^{2}}-1\right) e^{-\frac{i^{2}+j^{2}}{2 \sigma^{2}}}</script><p>该卷积mask的形状，在中心为正，而在附近为负，再远一点逐渐变为零。所以它也常常被称为<strong>墨西哥帽</strong>过滤器，见图6.12所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.12.png" alt="图6.12"></p>
<p>与普通边缘算子相比，该算子可以考虑更大的面积，但这并不总是件好事，因为并不知道该区域中是否存在许多边缘。此外，有时过多进行的平滑处理，会丢失诸如拐角之类的精细细节。另外，该算子给出了保证的边缘闭环（guaranteed closed loops of edges），这个优点很重要，但对某些应用程序（或某些类型的后处理）会造成问题。另外，神经生理学实验已经表明，人类的视觉系统（以神经节细胞的形式）执行的操作与Laplacian of Gaussian十分相似。<br>为了在实践中使用该滤波器，需要提出一种有效的应用方法，因为滤镜的mask通常很大（取决于$\sigma$的值），所以可以将二维的Laplacian of Gaussian函数分成四个一维的卷积操作，这样能计算的更快。另外，Laplacian of Gaussian通常用高斯的差（两个用不同的$\sigma$值平滑处理后的图像之间的差）来近似估计。<br>在二阶导数图像中找到zero crossings并不简单。我们不能只寻找零（因为域并不连续）。不过，如果一个点的符号（+ ve或–ve）与该行上的上一个点，或者该列中的上一个点的符号不同，将可以将它们标记为zero crossing。但是如果这样做，我们将舍弃二阶导数的主要优势之一（即，可以高精度的确定边缘位置）。</p>
<h4 id="Multiple-Scales"><a href="#Multiple-Scales" class="headerlink" title="Multiple Scales"></a>Multiple Scales</h4><p>许多图像处理技术要么使用局部的单个像素，要么使用一个局部区域中的频繁像素。但一个主要问题，是要知道使用多大的邻域，以及“正确”的领域大小取决于所调查对象的大小。这实际上是图像内的尺度问题。知道对象/物体是什么，就可以清楚地理解应该如何解释图像，这对于工业应用是可以接受的，因为工业要求固定，物体已知，但对于一般的视觉而言似乎是棘手的（鸡和鸡蛋类型的问题）。所以一种解决方案是同时研究不同分辨率（比例）下的图像。通过创建具有多种分辨率的模型，然后研究模型在不同分辨率下改变的方式，以获得在一个尺度上无法获取的元知识。<br>回到Laplacian of Gaussian，David Marr（Marr，1982）提出了一种可能性，即，应该使用不同的高斯算子对图像进行平滑处理，然后，跨多个尺度的对应关系可以被用来识别明显的不连续性（即，zero crossings）。有关在两个尺度下处理的图像的示例，见图6.13。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.13.png" alt="图6.13"></p>
<h4 id="Canny-Edge-Detection"><a href="#Canny-Edge-Detection" class="headerlink" title="Canny Edge Detection"></a>Canny Edge Detection</h4><p>Canny边缘检测器将一阶导数和二阶导数的边缘检测相结合，以计算边缘梯度和方向，它旨在优化以下三个标准：</p>
<ol>
<li>Detection - 边不能被遗漏。</li>
<li>Localisation - 实际的边缘和确定的边缘之间的距离应最小化。</li>
<li>One response - 对单个边的多个响应应最小化。</li>
</ol>
<p>具体算法：</p>
<ol>
<li>以标准偏差𝜎，来用高斯进行卷积增加模糊，去噪声。</li>
<li>使用高斯卷积后的图像的一阶导数，去估算边的法向量方向（即orientation）。</li>
<li>找到边，抑制non-maxima值。首先在高斯卷积图像的二阶导数中搜索zero-crossings得到准确的边缘信息，再通过上一步中计算出的边缘方向来抑制非最大值。</li>
<li>边的梯度是根据高斯卷积图像的一阶导数计算的。</li>
<li>利用hysteresis、磁滞现象来对边缘进行阈值处理。这里的设想是边缘点是存在于连接着的点的轮廓中的。那么使用两个阈值 – 高梯度阈值（在该阈值上所有点都明确地分类为边缘点）以及低阈值，在该阈值下且与确定为边缘点的点连接的点，也分类为边缘点。这样能解决那些，由于阈值太简单，而将边缘轮廓分裂成多个未连接的片段的问题。</li>
<li>与Marr的探测器类似，我们也可以针对多个尺度（使用不同的高斯）来处理，然后使用“feature synthesis、特征合成”来将不同尺度下的边组合到一起。</li>
</ol>
<p>Canny检测的边缘图如图6.14所示。</p>
<p><img src="/2020/08/06/读书笔记-计算机视觉入门介绍/6.14.png" alt="图6.14"></p>

      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/计算机视觉/" rel="tag"># 计算机视觉</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/09/28/Dynamic-Programming/" rel="next" title="Dynamic Programming">
                <i class="fa fa-chevron-left"></i> Dynamic Programming
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">29</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Difficult-Problem"><span class="nav-number">1.1.</span> <span class="nav-text">A Difficult Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Human-Vision-System"><span class="nav-number">1.2.</span> <span class="nav-text">The Human Vision System</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Practical-Applications-of-Computer-Vision"><span class="nav-number">1.3.</span> <span class="nav-text">Practical Applications of Computer Vision</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Future-of-Computer-Vision"><span class="nav-number">1.4.</span> <span class="nav-text">The Future of Computer Vision</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Images"><span class="nav-number">2.</span> <span class="nav-text">Images</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cameras"><span class="nav-number">2.1.</span> <span class="nav-text">Cameras</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Simple-Pinhole-Camera-Model"><span class="nav-number">2.1.1.</span> <span class="nav-text">The Simple Pinhole Camera Model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Images-1"><span class="nav-number">2.2.</span> <span class="nav-text">Images</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sampling"><span class="nav-number">2.2.1.</span> <span class="nav-text">Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quantisation"><span class="nav-number">2.2.2.</span> <span class="nav-text">Quantisation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Colour-Images"><span class="nav-number">2.3.</span> <span class="nav-text">Colour Images</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Red–Green–Blue-RGB-Images"><span class="nav-number">2.3.1.</span> <span class="nav-text">Red–Green–Blue (RGB) Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cyan–Magenta–Yellow-CMY-Images"><span class="nav-number">2.3.2.</span> <span class="nav-text">Cyan–Magenta–Yellow (CMY) Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YUV-Images"><span class="nav-number">2.3.3.</span> <span class="nav-text">YUV Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hue-Luminance-Saturation-HLS-Images"><span class="nav-number">2.3.4.</span> <span class="nav-text">Hue Luminance Saturation (HLS) Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-Colour-Spaces"><span class="nav-number">2.3.5.</span> <span class="nav-text">Other Colour Spaces</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Some-Colour-Applications"><span class="nav-number">2.3.6.</span> <span class="nav-text">Some Colour Applications</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Skin-Detection"><span class="nav-number">2.3.6.1.</span> <span class="nav-text">Skin Detection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Red-Eye-Detection"><span class="nav-number">2.3.6.2.</span> <span class="nav-text">Red Eye Detection</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Noise"><span class="nav-number">2.4.</span> <span class="nav-text">Noise</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Types-of-Noise"><span class="nav-number">2.4.1.</span> <span class="nav-text">Types of Noise</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Gaussian-Noise"><span class="nav-number">2.4.1.1.</span> <span class="nav-text">Gaussian Noise</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Salt-and-Pepper-Noise"><span class="nav-number">2.4.1.2.</span> <span class="nav-text">Salt and Pepper Noise</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Noise-Models"><span class="nav-number">2.4.2.</span> <span class="nav-text">Noise Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Additive-Noise"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">Additive Noise</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiplicative-Noise"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">Multiplicative Noise</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Noise-Generation"><span class="nav-number">2.4.3.</span> <span class="nav-text">Noise Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Noise-Evaluation"><span class="nav-number">2.4.4.</span> <span class="nav-text">Noise Evaluation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Smoothing"><span class="nav-number">2.5.</span> <span class="nav-text">Smoothing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Averaging"><span class="nav-number">2.5.1.</span> <span class="nav-text">Image Averaging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Local-Averaging-and-Gaussian-Smoothing"><span class="nav-number">2.5.2.</span> <span class="nav-text">Local Averaging and Gaussian Smoothing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rotating-Mask"><span class="nav-number">2.5.3.</span> <span class="nav-text">Rotating Mask</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Median-Filter"><span class="nav-number">2.5.4.</span> <span class="nav-text">Median Filter</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Histograms"><span class="nav-number">3.</span> <span class="nav-text">Histograms</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1D-Histograms"><span class="nav-number">3.1.</span> <span class="nav-text">1D Histograms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Histogram-Smoothing"><span class="nav-number">3.1.1.</span> <span class="nav-text">Histogram Smoothing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Colour-Histograms"><span class="nav-number">3.1.2.</span> <span class="nav-text">Colour Histograms</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3D-Histograms"><span class="nav-number">3.2.</span> <span class="nav-text">3D Histograms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Histogram-Image-Equalisation"><span class="nav-number">3.3.</span> <span class="nav-text">Histogram/Image Equalisation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Histogram-Comparison"><span class="nav-number">3.4.</span> <span class="nav-text">Histogram Comparison</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Back-projection"><span class="nav-number">3.5.</span> <span class="nav-text">Back-projection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-means-Clustering"><span class="nav-number">3.6.</span> <span class="nav-text">k-means Clustering</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Binary-Vision"><span class="nav-number">4.</span> <span class="nav-text">Binary Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Thresholding"><span class="nav-number">4.1.</span> <span class="nav-text">Thresholding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Thresholding-Problems"><span class="nav-number">4.1.1.</span> <span class="nav-text">Thresholding Problems</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Threshold-Detection-Methods"><span class="nav-number">4.2.</span> <span class="nav-text">Threshold Detection Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Bimodal-Histogram-Analysis"><span class="nav-number">4.2.1.</span> <span class="nav-text">Bimodal Histogram Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimal-Thresholding"><span class="nav-number">4.2.2.</span> <span class="nav-text">Optimal Thresholding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Otsu-Thresholding"><span class="nav-number">4.2.3.</span> <span class="nav-text">Otsu Thresholding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Variations-on-Thresholding"><span class="nav-number">4.3.</span> <span class="nav-text">Variations on Thresholding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaptive-Thresholding"><span class="nav-number">4.3.1.</span> <span class="nav-text">Adaptive Thresholding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Band-Thresholding"><span class="nav-number">4.3.2.</span> <span class="nav-text">Band Thresholding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Semi-Thresholding"><span class="nav-number">4.3.3.</span> <span class="nav-text">Semi-Thresholding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multispectral-Thresholding"><span class="nav-number">4.3.4.</span> <span class="nav-text">Multispectral Thresholding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mathematical-Morphology"><span class="nav-number">4.4.</span> <span class="nav-text">Mathematical Morphology</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dilation"><span class="nav-number">4.4.1.</span> <span class="nav-text">Dilation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Erosion"><span class="nav-number">4.4.2.</span> <span class="nav-text">Erosion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Opening-and-Closing"><span class="nav-number">4.4.3.</span> <span class="nav-text">Opening and Closing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Grey-Scale-and-Colour-Morphology"><span class="nav-number">4.4.4.</span> <span class="nav-text">Grey-Scale and Colour Morphology</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Connectivity"><span class="nav-number">4.5.</span> <span class="nav-text">Connectivity</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Connectedness-Paradoxes-and-Solutions"><span class="nav-number">4.5.1.</span> <span class="nav-text">Connectedness: Paradoxes and Solutions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Connected-Components-Analysis"><span class="nav-number">4.5.2.</span> <span class="nav-text">Connected Components Analysis</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Geometric-Transformations"><span class="nav-number">5.</span> <span class="nav-text">Geometric Transformations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem-Specification-and-Algorithm"><span class="nav-number">5.1.</span> <span class="nav-text">Problem Specification and Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Affine-Transformations"><span class="nav-number">5.2.</span> <span class="nav-text">Affine Transformations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Known-Affine-Transformations"><span class="nav-number">5.2.1.</span> <span class="nav-text">Known Affine Transformations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Translation"><span class="nav-number">5.2.1.1.</span> <span class="nav-text">Translation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Change-of-Scale-Expand-Shrink"><span class="nav-number">5.2.1.2.</span> <span class="nav-text">Change of Scale (Expand/Shrink)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Rotation"><span class="nav-number">5.2.1.3.</span> <span class="nav-text">Rotation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Skewing"><span class="nav-number">5.2.1.4.</span> <span class="nav-text">Skewing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Panoramic-Distortion"><span class="nav-number">5.2.1.5.</span> <span class="nav-text">Panoramic Distortion</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unknown-Affine-Transformations"><span class="nav-number">5.2.2.</span> <span class="nav-text">Unknown Affine Transformations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Perspective-Transformations"><span class="nav-number">5.3.</span> <span class="nav-text">Perspective Transformations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Specification-of-More-Complex-Transformations"><span class="nav-number">5.4.</span> <span class="nav-text">Specification of More Complex Transformations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Interpolation"><span class="nav-number">5.5.</span> <span class="nav-text">Interpolation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Nearest-Neighbour-Interpolation"><span class="nav-number">5.5.1.</span> <span class="nav-text">Nearest Neighbour Interpolation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bilinear-Interpolation"><span class="nav-number">5.5.2.</span> <span class="nav-text">Bilinear Interpolation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bi-Cubic-Interpolation"><span class="nav-number">5.5.3.</span> <span class="nav-text">Bi-Cubic Interpolation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Modelling-and-Removing-Distortion-from-Cameras"><span class="nav-number">5.6.</span> <span class="nav-text">Modelling and Removing Distortion from Cameras</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Camera-Distortions"><span class="nav-number">5.6.1.</span> <span class="nav-text">Camera Distortions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Camera-Calibration-and-Removing-Distortion"><span class="nav-number">5.6.2.</span> <span class="nav-text">Camera Calibration and Removing Distortion</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Edges"><span class="nav-number">6.</span> <span class="nav-text">Edges</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Edge-Detection"><span class="nav-number">6.1.</span> <span class="nav-text">Edge Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#First-Derivative-Edge-Detectors"><span class="nav-number">6.1.1.</span> <span class="nav-text">First Derivative Edge Detectors</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Roberts-Edge-Detector"><span class="nav-number">6.1.1.1.</span> <span class="nav-text">Roberts Edge Detector</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Compass-Edge-Detectors"><span class="nav-number">6.1.1.2.</span> <span class="nav-text">Compass Edge Detectors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-Gradient-and-Orientation"><span class="nav-number">6.1.1.3.</span> <span class="nav-text">Computing Gradient and Orientation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Edge-Image-Thresholding"><span class="nav-number">6.1.1.4.</span> <span class="nav-text">Edge Image Thresholding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Non-maxima-Suppression"><span class="nav-number">6.1.1.5.</span> <span class="nav-text">Non-maxima Suppression</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Second-Derivative-Edge-Detectors"><span class="nav-number">6.1.2.</span> <span class="nav-text">Second Derivative Edge Detectors</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Laplacian-of-Gaussian"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">Laplacian of Gaussian</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiple-Scales"><span class="nav-number">6.1.2.2.</span> <span class="nav-text">Multiple Scales</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Canny-Edge-Detection"><span class="nav-number">6.1.2.3.</span> <span class="nav-text">Canny Edge Detection</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>



  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  

  


  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  

  


  

</body>
</html>
